{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Evaluation Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/my_approach.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Document Preparation**: Load the entire document and extract content.\n",
    "2. **Question Generation**: Use an LLM to generate broad and detailed questions about the document.\n",
    "3. **Chunking Application**: Apply various chunking methods to the document. The next steps will be executed for each chunking strategy separately.\n",
    "4. **Chunk-Question Relevancy Scoring**: For each generated question, instruct an LLM to grade all chunks by relevancy to the question. These chunks become the ground truth and will represent our silver evaluation dataset.\n",
    "5. **Human Annotation**: Human annotators then review and modify the silver dataset to produce the gold dataset.\n",
    "6. **Chunk Retrieval**: Embed the chunks and retrieve the most similar chunks to each question.\n",
    "7. **Evaluate Retrieval**: Compare the retrieved chunks with the synthesized ground truth chunks and calculate the Precision, Recall and nDCG to analyse the retrieval performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, TypedDict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_dotenv(override=True)\n",
    "\n",
    "data_dir = \"my_benchmark/\"\n",
    "os.environ['CHUNKING_BENCHMARK'] = data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and Save Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document is loaded as one Langchain document possibly to small to fit into a LLM. Therefore, we need to split these documents into smaller pieces of text for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import save_documents\n",
    "\n",
    "documents: List[Document] = []\n",
    "for file in os.listdir(data_dir+\"documents\"):\n",
    "    file_path = os.path.join(data_dir+\"documents\", file)\n",
    "    loader = TextLoader(file_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "save_documents(documents, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Question Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate synthetic Questions across Documents to challenge chunking strategies on multi-context queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_documents\n",
    "documents = load_documents(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question generation based on documents\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str = Field(description=\"The question generated by the model\")\n",
    "    type: str = Field(description=\"The type of question generated\")\n",
    "\n",
    "class Questions(BaseModel):\n",
    "    questions: List[Question] = Field(description=\"The list of questions generated by the model\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Questions)\n",
    "\n",
    "question_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"document\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    template=\"\"\"\n",
    "You are a highly knowledgeable assistant tasked with generating challenging questions to evaluate different document chunking strategies in retrieval augmented generation (RAG) pipelines.\n",
    "Your goal is to create questions that require detailed, specific, and nuanced understanding of the document content.\n",
    "These questions should test the ability of different chunking strategies to retrieve and generate accurate and comprehensive responses.\n",
    "\n",
    "Here is the document:\n",
    "{document}\n",
    "\n",
    "Based on the document provided, generate a set of questions that meet the following criteria:\n",
    "\n",
    "1. **Complexity:** Questions should be complex and require in-depth understanding of the document, involving multiple facts or concepts interlinked within the document.\n",
    "2. **Specificity:** Questions should be specific and precise, targeting particular sections or details within the document.\n",
    "3. **Inference:** Questions should require inferential reasoning, where the answer is not directly stated but can be deduced from the document content.\n",
    "4. **Variability:** Include a mix of question types, such as:\n",
    "   - Conceptual questions (e.g., understanding the main ideas or arguments presented)\n",
    "   - Analytical questions (e.g., comparing or contrasting information within the document)\n",
    "   - Application questions (e.g., applying the information or concepts to a hypothetical scenario)\n",
    "   - Synthesis questions (e.g., combining multiple pieces of information to form a comprehensive answer)\n",
    "\n",
    "Ensure that the questions are designed to challenge the document retrieval and generation capabilities of different chunking strategies.\n",
    "\n",
    "Please generate 4 such challenging questions based on the provided document as follows:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = question_generation_prompt | generator_llm | parser\n",
    "questions = []\n",
    "for document in documents[:4]:\n",
    "    doc_questions = chain.invoke({\"document\": document.page_content})[\"questions\"]\n",
    "    for question in doc_questions:\n",
    "        question[\"source\"] = document.metadata[\"source\"]\n",
    "\n",
    "    questions.extend(doc_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_dir}synthetic_questions.json\", \"w\") as f:\n",
    "    json.dump(questions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Apply chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i chunking_strategies.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_chunks\n",
    "split_chunks: Dict[str, Document] = load_chunks(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ea16f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_ea16f_level0_col0\" class=\"col_heading level0 col0\" >Experiment</th>\n",
       "      <th id=\"T_ea16f_level0_col1\" class=\"col_heading level0 col1\" >Chunk Count</th>\n",
       "      <th id=\"T_ea16f_level0_col2\" class=\"col_heading level0 col2\" >Average Chunk Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row0_col0\" class=\"data row0 col0\" >fixed_size-2048-0</td>\n",
       "      <td id=\"T_ea16f_row0_col1\" class=\"data row0 col1\" >65</td>\n",
       "      <td id=\"T_ea16f_row0_col2\" class=\"data row0 col2\" >1915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row1_col0\" class=\"data row1 col0\" >semantic_chunks_95</td>\n",
       "      <td id=\"T_ea16f_row1_col1\" class=\"data row1 col1\" >67</td>\n",
       "      <td id=\"T_ea16f_row1_col2\" class=\"data row1 col2\" >1848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row2_col0\" class=\"data row2 col0\" >fixed_size-2048-200</td>\n",
       "      <td id=\"T_ea16f_row2_col1\" class=\"data row2 col1\" >72</td>\n",
       "      <td id=\"T_ea16f_row2_col2\" class=\"data row2 col2\" >1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row3_col0\" class=\"data row3 col0\" >recursive-2048-0</td>\n",
       "      <td id=\"T_ea16f_row3_col1\" class=\"data row3 col1\" >72</td>\n",
       "      <td id=\"T_ea16f_row3_col2\" class=\"data row3 col2\" >1728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row4_col0\" class=\"data row4 col0\" >recursive-2048-200</td>\n",
       "      <td id=\"T_ea16f_row4_col1\" class=\"data row4 col1\" >73</td>\n",
       "      <td id=\"T_ea16f_row4_col2\" class=\"data row4 col2\" >1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row5_col0\" class=\"data row5 col0\" >semantic_chunks_90</td>\n",
       "      <td id=\"T_ea16f_row5_col1\" class=\"data row5 col1\" >118</td>\n",
       "      <td id=\"T_ea16f_row5_col2\" class=\"data row5 col2\" >1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row6_col0\" class=\"data row6 col0\" >fixed_size-1024-0</td>\n",
       "      <td id=\"T_ea16f_row6_col1\" class=\"data row6 col1\" >127</td>\n",
       "      <td id=\"T_ea16f_row6_col2\" class=\"data row6 col2\" >980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row7_col0\" class=\"data row7 col0\" >markdown_header</td>\n",
       "      <td id=\"T_ea16f_row7_col1\" class=\"data row7 col1\" >146</td>\n",
       "      <td id=\"T_ea16f_row7_col2\" class=\"data row7 col2\" >844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row8_col0\" class=\"data row8 col0\" >markdown_header_parent</td>\n",
       "      <td id=\"T_ea16f_row8_col1\" class=\"data row8 col1\" >146</td>\n",
       "      <td id=\"T_ea16f_row8_col2\" class=\"data row8 col2\" >873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row9_col0\" class=\"data row9 col0\" >fixed_size-1024-200</td>\n",
       "      <td id=\"T_ea16f_row9_col1\" class=\"data row9 col1\" >153</td>\n",
       "      <td id=\"T_ea16f_row9_col2\" class=\"data row9 col2\" >1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row10_col0\" class=\"data row10 col0\" >recursive-1024-0</td>\n",
       "      <td id=\"T_ea16f_row10_col1\" class=\"data row10 col1\" >155</td>\n",
       "      <td id=\"T_ea16f_row10_col2\" class=\"data row10 col2\" >802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row11_col0\" class=\"data row11 col0\" >recursive-1024-200</td>\n",
       "      <td id=\"T_ea16f_row11_col1\" class=\"data row11 col1\" >164</td>\n",
       "      <td id=\"T_ea16f_row11_col2\" class=\"data row11 col2\" >810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row12_col0\" class=\"data row12 col0\" >fixed_size-512-0</td>\n",
       "      <td id=\"T_ea16f_row12_col1\" class=\"data row12 col1\" >248</td>\n",
       "      <td id=\"T_ea16f_row12_col2\" class=\"data row12 col2\" >502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row13_col0\" class=\"data row13 col0\" >recursive-512-0</td>\n",
       "      <td id=\"T_ea16f_row13_col1\" class=\"data row13 col1\" >353</td>\n",
       "      <td id=\"T_ea16f_row13_col2\" class=\"data row13 col2\" >351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row14_col0\" class=\"data row14 col0\" >recursive-512-200</td>\n",
       "      <td id=\"T_ea16f_row14_col1\" class=\"data row14 col1\" >379</td>\n",
       "      <td id=\"T_ea16f_row14_col2\" class=\"data row14 col2\" >378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ea16f_row15_col0\" class=\"data row15 col0\" >fixed_size-512-200</td>\n",
       "      <td id=\"T_ea16f_row15_col1\" class=\"data row15 col1\" >398</td>\n",
       "      <td id=\"T_ea16f_row15_col2\" class=\"data row15 col2\" >507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7efd4c0da530>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"Experiment\", \"Chunk Count\", \"Average Chunk Size\"])\n",
    "for experiment_name, chunks in split_chunks.items():\n",
    "    df.loc[len(df)] = [experiment_name, len(chunks), round(sum([len(chunk.page_content) for chunk in chunks])/len(chunks))]\n",
    "\n",
    "df.sort_values(by=\"Chunk Count\", ascending=True).style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. (5.) Create Evaluation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Relevancy Score for each chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_dir}synthetic_questions.json\", \"r\") as f:\n",
    "    questions = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize datasets with questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import Testset\n",
    "\n",
    "datasets: Dict[str, List[Testset]]  = {}\n",
    "for experiment_name in split_chunks.keys():\n",
    "    datasets[experiment_name] = []\n",
    "    for question in questions:\n",
    "        datasets[experiment_name].append({\n",
    "            \"question\": question[\"question\"],\n",
    "            \"source\": question[\"source\"],\n",
    "            \"type\": question[\"type\"],\n",
    "            \"ground_truth_chunks\": {} \n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevancy Prompt is taken by Trulens. The difference is that I apply it to all chunks whereas Trulens only computed it on the retrieved chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for markdown_header_parent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:16<00:00,  3.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from utils.llm_output_parser import re_0_10_rating\n",
    "\n",
    "system_prompt = \"\"\"You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\n",
    "    Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
    "\n",
    "    A few additional scoring guidelines:\n",
    "\n",
    "    - Long CONTEXTS should score equally well as short CONTEXTS.\n",
    "\n",
    "    - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\n",
    "\n",
    "    - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
    "\n",
    "    - Never elaborate.\"\"\"\n",
    "\n",
    "user_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"QUESTION: {question}\n",
    "\n",
    "    CONTEXT: {context}\n",
    "    \n",
    "    RELEVANCE: \"\"\"\n",
    ")\n",
    "\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def make_request_with_backoff(messages, retries=8):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = critic_llm.invoke(messages)\n",
    "            return response\n",
    "        except openai.RateLimitError as e:\n",
    "            if i == retries - 1:\n",
    "                raise e\n",
    "            wait_time = 2**i\n",
    "            print(f\"Rate limited, waiting {wait_time} seconds\")\n",
    "            time.sleep(wait_time)\n",
    "        except openai.APIError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def process_chunk(chunk, testset):\n",
    "    if chunk.metadata[\"source\"] not in testset[\"source\"]:\n",
    "        return None, None\n",
    "\n",
    "    judge_chunk_relevancy_prompt = user_prompt.format(\n",
    "        question=testset[\"question\"], context=chunk.page_content\n",
    "    )\n",
    "\n",
    "    llm_messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=judge_chunk_relevancy_prompt),\n",
    "    ]\n",
    "    response = make_request_with_backoff(llm_messages)\n",
    "    chunk_relevancy = re_0_10_rating(response.content)\n",
    "    if chunk_relevancy != 0.0:\n",
    "        return str(chunk.metadata[\"id\"]), chunk_relevancy\n",
    "    return None, None\n",
    "\n",
    "for experiment_name, questions in datasets.items():\n",
    "    if os.path.exists(f\"{data_dir}/datasets/{experiment_name}.json\"):\n",
    "        continue\n",
    "\n",
    "    print(\"Collecting ground truth for\", experiment_name)\n",
    "    for testset in tqdm(questions):\n",
    "        ground_truth = {}\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            future_to_chunk = {\n",
    "                executor.submit(process_chunk, chunk, testset): chunk\n",
    "                for chunk in split_chunks[experiment_name]\n",
    "            }\n",
    "            for future in as_completed(future_to_chunk):\n",
    "                chunk_id, relevancy = future.result()\n",
    "                if chunk_id and relevancy:\n",
    "                    ground_truth[chunk_id] = relevancy\n",
    "        \n",
    "        if len(ground_truth):\n",
    "            testset[\"ground_truth_chunks\"] = ground_truth\n",
    "\n",
    "    with open(f\"{data_dir}/datasets/{experiment_name}.json\", \"w\") as f:\n",
    "        json.dump(questions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_2cd38\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_2cd38_level0_col0\" class=\"col_heading level0 col0\" >Experiment</th>\n",
       "      <th id=\"T_2cd38_level0_col1\" class=\"col_heading level0 col1\" >Chunk Count</th>\n",
       "      <th id=\"T_2cd38_level0_col2\" class=\"col_heading level0 col2\" >Average Chunk Size</th>\n",
       "      <th id=\"T_2cd38_level0_col3\" class=\"col_heading level0 col3\" >Average Ground Truth Count</th>\n",
       "      <th id=\"T_2cd38_level0_col4\" class=\"col_heading level0 col4\" >Average Relevancy per Ground Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row0_col0\" class=\"data row0 col0\" >fixed_size-2048-0</td>\n",
       "      <td id=\"T_2cd38_row0_col1\" class=\"data row0 col1\" >65</td>\n",
       "      <td id=\"T_2cd38_row0_col2\" class=\"data row0 col2\" >1915</td>\n",
       "      <td id=\"T_2cd38_row0_col3\" class=\"data row0 col3\" >6.450000</td>\n",
       "      <td id=\"T_2cd38_row0_col4\" class=\"data row0 col4\" >4.806202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row1_col0\" class=\"data row1 col0\" >fixed_size-2048-200</td>\n",
       "      <td id=\"T_2cd38_row1_col1\" class=\"data row1 col1\" >72</td>\n",
       "      <td id=\"T_2cd38_row1_col2\" class=\"data row1 col2\" >1902</td>\n",
       "      <td id=\"T_2cd38_row1_col3\" class=\"data row1 col3\" >6.950000</td>\n",
       "      <td id=\"T_2cd38_row1_col4\" class=\"data row1 col4\" >4.784173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row2_col0\" class=\"data row2 col0\" >semantic_chunks_95</td>\n",
       "      <td id=\"T_2cd38_row2_col1\" class=\"data row2 col1\" >67</td>\n",
       "      <td id=\"T_2cd38_row2_col2\" class=\"data row2 col2\" >1848</td>\n",
       "      <td id=\"T_2cd38_row2_col3\" class=\"data row2 col3\" >6.150000</td>\n",
       "      <td id=\"T_2cd38_row2_col4\" class=\"data row2 col4\" >5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row3_col0\" class=\"data row3 col0\" >recursive-2048-200</td>\n",
       "      <td id=\"T_2cd38_row3_col1\" class=\"data row3 col1\" >73</td>\n",
       "      <td id=\"T_2cd38_row3_col2\" class=\"data row3 col2\" >1736</td>\n",
       "      <td id=\"T_2cd38_row3_col3\" class=\"data row3 col3\" >7.150000</td>\n",
       "      <td id=\"T_2cd38_row3_col4\" class=\"data row3 col4\" >4.888112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row4_col0\" class=\"data row4 col0\" >recursive-2048-0</td>\n",
       "      <td id=\"T_2cd38_row4_col1\" class=\"data row4 col1\" >72</td>\n",
       "      <td id=\"T_2cd38_row4_col2\" class=\"data row4 col2\" >1728</td>\n",
       "      <td id=\"T_2cd38_row4_col3\" class=\"data row4 col3\" >6.950000</td>\n",
       "      <td id=\"T_2cd38_row4_col4\" class=\"data row4 col4\" >5.100719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row5_col0\" class=\"data row5 col0\" >semantic_chunks_90</td>\n",
       "      <td id=\"T_2cd38_row5_col1\" class=\"data row5 col1\" >118</td>\n",
       "      <td id=\"T_2cd38_row5_col2\" class=\"data row5 col2\" >1049</td>\n",
       "      <td id=\"T_2cd38_row5_col3\" class=\"data row5 col3\" >11.000000</td>\n",
       "      <td id=\"T_2cd38_row5_col4\" class=\"data row5 col4\" >4.172727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row6_col0\" class=\"data row6 col0\" >fixed_size-1024-200</td>\n",
       "      <td id=\"T_2cd38_row6_col1\" class=\"data row6 col1\" >153</td>\n",
       "      <td id=\"T_2cd38_row6_col2\" class=\"data row6 col2\" >1000</td>\n",
       "      <td id=\"T_2cd38_row6_col3\" class=\"data row6 col3\" >15.400000</td>\n",
       "      <td id=\"T_2cd38_row6_col4\" class=\"data row6 col4\" >3.967532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row7_col0\" class=\"data row7 col0\" >fixed_size-1024-0</td>\n",
       "      <td id=\"T_2cd38_row7_col1\" class=\"data row7 col1\" >127</td>\n",
       "      <td id=\"T_2cd38_row7_col2\" class=\"data row7 col2\" >980</td>\n",
       "      <td id=\"T_2cd38_row7_col3\" class=\"data row7 col3\" >12.600000</td>\n",
       "      <td id=\"T_2cd38_row7_col4\" class=\"data row7 col4\" >3.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row8_col0\" class=\"data row8 col0\" >markdown_header_parent</td>\n",
       "      <td id=\"T_2cd38_row8_col1\" class=\"data row8 col1\" >146</td>\n",
       "      <td id=\"T_2cd38_row8_col2\" class=\"data row8 col2\" >873</td>\n",
       "      <td id=\"T_2cd38_row8_col3\" class=\"data row8 col3\" >14.350000</td>\n",
       "      <td id=\"T_2cd38_row8_col4\" class=\"data row8 col4\" >3.662021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row9_col0\" class=\"data row9 col0\" >markdown_header</td>\n",
       "      <td id=\"T_2cd38_row9_col1\" class=\"data row9 col1\" >146</td>\n",
       "      <td id=\"T_2cd38_row9_col2\" class=\"data row9 col2\" >844</td>\n",
       "      <td id=\"T_2cd38_row9_col3\" class=\"data row9 col3\" >14.100000</td>\n",
       "      <td id=\"T_2cd38_row9_col4\" class=\"data row9 col4\" >3.723404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row10_col0\" class=\"data row10 col0\" >recursive-1024-200</td>\n",
       "      <td id=\"T_2cd38_row10_col1\" class=\"data row10 col1\" >164</td>\n",
       "      <td id=\"T_2cd38_row10_col2\" class=\"data row10 col2\" >810</td>\n",
       "      <td id=\"T_2cd38_row10_col3\" class=\"data row10 col3\" >16.300000</td>\n",
       "      <td id=\"T_2cd38_row10_col4\" class=\"data row10 col4\" >3.987730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row11_col0\" class=\"data row11 col0\" >recursive-1024-0</td>\n",
       "      <td id=\"T_2cd38_row11_col1\" class=\"data row11 col1\" >155</td>\n",
       "      <td id=\"T_2cd38_row11_col2\" class=\"data row11 col2\" >802</td>\n",
       "      <td id=\"T_2cd38_row11_col3\" class=\"data row11 col3\" >15.400000</td>\n",
       "      <td id=\"T_2cd38_row11_col4\" class=\"data row11 col4\" >4.048701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row12_col0\" class=\"data row12 col0\" >fixed_size-512-200</td>\n",
       "      <td id=\"T_2cd38_row12_col1\" class=\"data row12 col1\" >398</td>\n",
       "      <td id=\"T_2cd38_row12_col2\" class=\"data row12 col2\" >507</td>\n",
       "      <td id=\"T_2cd38_row12_col3\" class=\"data row12 col3\" >39.600000</td>\n",
       "      <td id=\"T_2cd38_row12_col4\" class=\"data row12 col4\" >3.175505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row13_col0\" class=\"data row13 col0\" >fixed_size-512-0</td>\n",
       "      <td id=\"T_2cd38_row13_col1\" class=\"data row13 col1\" >248</td>\n",
       "      <td id=\"T_2cd38_row13_col2\" class=\"data row13 col2\" >502</td>\n",
       "      <td id=\"T_2cd38_row13_col3\" class=\"data row13 col3\" >24.250000</td>\n",
       "      <td id=\"T_2cd38_row13_col4\" class=\"data row13 col4\" >3.169072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row14_col0\" class=\"data row14 col0\" >recursive-512-200</td>\n",
       "      <td id=\"T_2cd38_row14_col1\" class=\"data row14 col1\" >379</td>\n",
       "      <td id=\"T_2cd38_row14_col2\" class=\"data row14 col2\" >378</td>\n",
       "      <td id=\"T_2cd38_row14_col3\" class=\"data row14 col3\" >36.800000</td>\n",
       "      <td id=\"T_2cd38_row14_col4\" class=\"data row14 col4\" >3.327446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2cd38_row15_col0\" class=\"data row15 col0\" >recursive-512-0</td>\n",
       "      <td id=\"T_2cd38_row15_col1\" class=\"data row15 col1\" >353</td>\n",
       "      <td id=\"T_2cd38_row15_col2\" class=\"data row15 col2\" >351</td>\n",
       "      <td id=\"T_2cd38_row15_col3\" class=\"data row15 col3\" >33.600000</td>\n",
       "      <td id=\"T_2cd38_row15_col4\" class=\"data row15 col4\" >3.297619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5a7ce9ca90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.loader import load_datasets\n",
    "datasets = load_datasets(data_dir)\n",
    "\n",
    "df = pd.DataFrame(columns=[ \"Experiment\", \"Chunk Count\", \"Average Chunk Size\", \"Average Ground Truth Count\",\"Average Relevancy per Ground Truth\"])\n",
    "for experiment_name, questions in datasets.items():\n",
    "    total_relevancy = 0\n",
    "    total_ground_truth_count = 0\n",
    "    for question in questions:\n",
    "        for chunk_relevancy in question[\"ground_truth_chunks\"].values():\n",
    "            total_relevancy += chunk_relevancy\n",
    "        total_ground_truth_count += len(question[\"ground_truth_chunks\"])\n",
    "    \n",
    "    average_ground_truth_count = total_ground_truth_count / len(questions)\n",
    "    average_relevancy_per_ground_truth = total_relevancy / total_ground_truth_count\n",
    "    average_chunk_size = round(sum([len(chunk.page_content) for chunk in split_chunks[experiment_name]]) / len(split_chunks[experiment_name]))\n",
    "    df.loc[len(df)] = [experiment_name, len(split_chunks[experiment_name]), average_chunk_size, average_ground_truth_count, average_relevancy_per_ground_truth]\n",
    "\n",
    "df.sort_values(by=\"Average Chunk Size\", ascending=False).style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Total Ground Truth Token Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3343"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_ground_truth_token_count = 0\n",
    "for experiment_name, questions in datasets.items():\n",
    "    ground_character_token_count = 0\n",
    "    for question in questions:\n",
    "        for split_chunk in split_chunks[experiment_name]:\n",
    "            if str(split_chunk.metadata[\"id\"]) in question[\"ground_truth_chunks\"]:\n",
    "                ground_character_token_count += len(split_chunk.page_content)\n",
    "    mean_ground_truth_token_count += ground_character_token_count / (len(questions)*4) # *4 because on token is approximately 4 characters\n",
    "\n",
    "round(mean_ground_truth_token_count / len(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. - 7. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Chunks into Vector Store\n",
    "\n",
    "Using FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fixed_size-512-200\n",
      "Loading fixed_size-1024-0\n",
      "Loading semantic_chunks_90\n",
      "Loading fixed_size-1024-200\n",
      "Loading markdown_header\n",
      "Loading fixed_size-2048-0\n",
      "Loading fixed_size-512-0\n",
      "Loading recursive-1024-200\n",
      "Loading markdown_header_parent\n",
      "Loading fixed_size-2048-200\n",
      "Loading recursive-2048-0\n",
      "Loading recursive-512-200\n",
      "Loading recursive-2048-200\n",
      "Loading recursive-1024-0\n",
      "Loading semantic_chunks_95\n",
      "Loading recursive-512-0\n"
     ]
    }
   ],
   "source": [
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "vector_stores: Dict[str, VectorStore] = {}\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"Snowflake/snowflake-arctic-embed-l\",\n",
    "    # model_name=\"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "#     model_kwargs={\"device\": 0, 'trust_remote_code': True},  # Comment out to use CPU\n",
    "# )\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "model_name = (embeddings.model_name if hasattr(embeddings, 'model_name') else embeddings.model).replace(\"/\", \"_\")\n",
    "vector_store_dir = f\"{data_dir}vector_stores/{model_name}\"\n",
    "Path(vector_store_dir).mkdir(parents=True, exist_ok=True)\n",
    "for experiment_name, chunks in split_chunks.items():\n",
    "    if os.path.exists(f\"{vector_store_dir}/{experiment_name}\"):\n",
    "        print(\"Loading\", experiment_name)\n",
    "        vector_stores[experiment_name] = FAISS.load_local(f\"{vector_store_dir}/{experiment_name}\", embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        print(\"Indexing\", experiment_name)\n",
    "        vector_stores[experiment_name] = FAISS.from_documents(chunks, embeddings)\n",
    "        vector_stores[experiment_name].save_local(f\"{vector_store_dir}/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Evaluation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_datasets\n",
    "datasets = load_datasets(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Evaluation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalApproach:\n",
    "    FIXED_K = \"Fixed-K\"\n",
    "    GROUND_TRUTH_K = \"Ground-Truth-K\"\n",
    "    TOKEN_LIMIT = \"Token-Limit\"\n",
    "    RATIO_K = \"Ratio-K\"\n",
    "\n",
    "SEL_APPROACH: EvalApproach = EvalApproach.RATIO_K\n",
    "FIXED_K = 20\n",
    "TOKEN_LIMIT = 3340\n",
    "RATIO_K = 0.05\n",
    "\n",
    "model_name = \"text-embedding-3-small\"\n",
    "eval_name = f\"{SEL_APPROACH}-{FIXED_K}-{model_name}\" if SEL_APPROACH == EvalApproach.FIXED_K else \"\"\n",
    "eval_name = f\"{SEL_APPROACH}-{model_name}\" if SEL_APPROACH == EvalApproach.GROUND_TRUTH_K else eval_name\n",
    "eval_name = f\"{SEL_APPROACH}-{TOKEN_LIMIT}-{model_name}\" if SEL_APPROACH == EvalApproach.TOKEN_LIMIT else eval_name\n",
    "eval_name = f\"{SEL_APPROACH}-{RATIO_K}-{model_name}\" if SEL_APPROACH == EvalApproach.RATIO_K else eval_name\n",
    "if os.path.exists(f\"{data_dir}results/{eval_name}.csv\"):\n",
    "    results = pd.read_csv(f\"{data_dir}results/{eval_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fixed_size-512-200 with K = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating semantic_chunks_90 with K = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recursive-2048-0 with K = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recursive-1024-200 with K = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating semantic_chunks_95 with K = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fixed_size-1024-200 with K = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recursive-2048-200 with K = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fixed_size-1024-0 with K = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recursive-1024-0 with K = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fixed_size-2048-0 with K = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recursive-512-200 with K = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recursive-512-0 with K = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fixed_size-512-0 with K = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating markdown_header with K = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating markdown_header_parent with K = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fixed_size-2048-200 with K = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.evaluation import calculate_metrics, calculate_mean_metrics\n",
    "\n",
    "results_list = []\n",
    "for experiment_name, questions in datasets.items():\n",
    "    if experiment_name not in vector_stores:\n",
    "        continue\n",
    "\n",
    "    K = FIXED_K if SEL_APPROACH == EvalApproach.FIXED_K else 0\n",
    "    K = (\n",
    "        round(len(split_chunks[experiment_name]) * RATIO_K)\n",
    "        if SEL_APPROACH == EvalApproach.RATIO_K\n",
    "        else K\n",
    "    )\n",
    "    K = (\n",
    "        200 if SEL_APPROACH == EvalApproach.TOKEN_LIMIT else K\n",
    "    )  # large number to ensure TOKEN_LIMIT is always reached\n",
    "    print(\"Evaluating\", experiment_name, \"with K =\", K if K else \"Ground Truth based\")\n",
    "    metrics = []\n",
    "    for testset in tqdm(questions):\n",
    "        if testset[\"ground_truth_chunks\"] == {}:\n",
    "            continue\n",
    "        question = testset[\"question\"]\n",
    "        ground_truth = testset[\"ground_truth_chunks\"]\n",
    "        K = len(ground_truth) if SEL_APPROACH == EvalApproach.GROUND_TRUTH_K else K\n",
    "\n",
    "        retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": K})\n",
    "        retrieved_chunks = retriever.invoke(question)\n",
    "\n",
    "        if SEL_APPROACH == EvalApproach.TOKEN_LIMIT:\n",
    "            # cap the number of retrieved chunks where sum of page_contents are below a fixed context window\n",
    "            retrieved_chunks_capped = []\n",
    "            total_context_length = 0\n",
    "            for chunk in retrieved_chunks:\n",
    "                total_context_length += len(chunk.page_content)\n",
    "                if (\n",
    "                    total_context_length > TOKEN_LIMIT * 4\n",
    "                ):  # as one token on average is approximately 4 characters\n",
    "                    break\n",
    "                retrieved_chunks_capped.append(chunk)\n",
    "\n",
    "            retrieved_chunks = retrieved_chunks_capped\n",
    "\n",
    "        retrieved_chunk_ids = [str(doc.metadata[\"id\"]) for doc in retrieved_chunks]\n",
    "        metrics.append(\n",
    "            calculate_metrics(\n",
    "                retrieved_chunk_ids,\n",
    "                ground_truth_chunks=list(ground_truth.keys()),\n",
    "                ground_truth_relevancies=list(ground_truth.values()),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    mean_metrics = (\n",
    "        calculate_mean_metrics(metrics)\n",
    "        if len(metrics)\n",
    "        else {\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"map\": 0.0,\n",
    "            \"ndcg\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        experiment_chunk_size = int(experiment_name.split(\"-\")[-2])\n",
    "        experiment_chunk_overlap = int(experiment_name.split(\"-\")[-1])\n",
    "    except:\n",
    "        experiment_chunk_size = None\n",
    "        experiment_chunk_overlap = None\n",
    "\n",
    "    results_list.append(\n",
    "        [\n",
    "            experiment_name.split(\"-\")[0],\n",
    "            experiment_chunk_size,\n",
    "            experiment_chunk_overlap,\n",
    "            mean_metrics[\"precision\"],\n",
    "            mean_metrics[\"recall\"],\n",
    "            mean_metrics[\"map\"],\n",
    "            mean_metrics[\"ndcg\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    results_list,\n",
    "    columns=[\n",
    "        eval_name,\n",
    "        \"Chunk Size\",\n",
    "        \"Chunk Overlap\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"MAP\",\n",
    "        \"NDCG\",\n",
    "    ],\n",
    ")\n",
    "results.to_csv(f\"{data_dir}results/{eval_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ratio-K-0.05-text-embedding-3-small</th>\n",
       "      <th>Chunk Size</th>\n",
       "      <th>Chunk Overlap</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>semantic_chunks_95</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.661667</td>\n",
       "      <td>0.888422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recursive</th>\n",
       "      <th>2048.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.627172</td>\n",
       "      <td>0.908906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_size</th>\n",
       "      <th>2048.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.625354</td>\n",
       "      <td>0.908574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recursive</th>\n",
       "      <th>2048.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.625354</td>\n",
       "      <td>0.908903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic_chunks_90</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.616118</td>\n",
       "      <td>0.915729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_size</th>\n",
       "      <th>1024.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.597243</td>\n",
       "      <td>0.889170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recursive</th>\n",
       "      <th>1024.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.586630</td>\n",
       "      <td>0.902028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.577365</td>\n",
       "      <td>0.898449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recursive</th>\n",
       "      <th>1024.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.576862</td>\n",
       "      <td>0.916890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_size</th>\n",
       "      <th>1024.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.565604</td>\n",
       "      <td>0.905585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header_parent</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.564560</td>\n",
       "      <td>0.897757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">fixed_size</th>\n",
       "      <th>2048.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.913379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.877500</td>\n",
       "      <td>0.558478</td>\n",
       "      <td>0.878468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">recursive</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">512.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.552869</td>\n",
       "      <td>0.842148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200.0</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.551683</td>\n",
       "      <td>0.838664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_size</th>\n",
       "      <th>512.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.547014</td>\n",
       "      <td>0.883378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Precision  \\\n",
       "Ratio-K-0.05-text-embedding-3-small Chunk Size Chunk Overlap              \n",
       "semantic_chunks_95                  NaN        NaN             1.000000   \n",
       "recursive                           2048.0     200.0           0.937500   \n",
       "fixed_size                          2048.0     200.0           0.875000   \n",
       "recursive                           2048.0     0.0             0.875000   \n",
       "semantic_chunks_90                  NaN        NaN             0.841667   \n",
       "fixed_size                          1024.0     200.0           0.906250   \n",
       "recursive                           1024.0     0.0             0.900000   \n",
       "markdown_header                     NaN        NaN             0.964286   \n",
       "recursive                           1024.0     200.0           0.900000   \n",
       "fixed_size                          1024.0     0.0             0.916667   \n",
       "markdown_header_parent              NaN        NaN             0.957143   \n",
       "fixed_size                          2048.0     0.0             0.916667   \n",
       "                                    512.0      200.0           0.877500   \n",
       "recursive                           512.0      0.0             0.855556   \n",
       "                                               200.0           0.850000   \n",
       "fixed_size                          512.0      0.0             0.887500   \n",
       "\n",
       "                                                                Recall  \\\n",
       "Ratio-K-0.05-text-embedding-3-small Chunk Size Chunk Overlap             \n",
       "semantic_chunks_95                  NaN        NaN            0.661667   \n",
       "recursive                           2048.0     200.0          0.627172   \n",
       "fixed_size                          2048.0     200.0          0.625354   \n",
       "recursive                           2048.0     0.0            0.625354   \n",
       "semantic_chunks_90                  NaN        NaN            0.616118   \n",
       "fixed_size                          1024.0     200.0          0.597243   \n",
       "recursive                           1024.0     0.0            0.586630   \n",
       "markdown_header                     NaN        NaN            0.577365   \n",
       "recursive                           1024.0     200.0          0.576862   \n",
       "fixed_size                          1024.0     0.0            0.565604   \n",
       "markdown_header_parent              NaN        NaN            0.564560   \n",
       "fixed_size                          2048.0     0.0            0.560000   \n",
       "                                    512.0      200.0          0.558478   \n",
       "recursive                           512.0      0.0            0.552869   \n",
       "                                               200.0          0.551683   \n",
       "fixed_size                          512.0      0.0            0.547014   \n",
       "\n",
       "                                                                  NDCG  \n",
       "Ratio-K-0.05-text-embedding-3-small Chunk Size Chunk Overlap            \n",
       "semantic_chunks_95                  NaN        NaN            0.888422  \n",
       "recursive                           2048.0     200.0          0.908906  \n",
       "fixed_size                          2048.0     200.0          0.908574  \n",
       "recursive                           2048.0     0.0            0.908903  \n",
       "semantic_chunks_90                  NaN        NaN            0.915729  \n",
       "fixed_size                          1024.0     200.0          0.889170  \n",
       "recursive                           1024.0     0.0            0.902028  \n",
       "markdown_header                     NaN        NaN            0.898449  \n",
       "recursive                           1024.0     200.0          0.916890  \n",
       "fixed_size                          1024.0     0.0            0.905585  \n",
       "markdown_header_parent              NaN        NaN            0.897757  \n",
       "fixed_size                          2048.0     0.0            0.913379  \n",
       "                                    512.0      200.0          0.878468  \n",
       "recursive                           512.0      0.0            0.842148  \n",
       "                                               200.0          0.838664  \n",
       "fixed_size                          512.0      0.0            0.883378  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.drop(columns=[\"MAP\"]).groupby([eval_name, \"Chunk Size\", \"Chunk Overlap\"], dropna=False).mean().sort_values(by=\"Recall\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Chunk Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chunk Size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2048.0</th>\n",
       "      <td>0.901042</td>\n",
       "      <td>0.609470</td>\n",
       "      <td>0.909941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.940774</td>\n",
       "      <td>0.604927</td>\n",
       "      <td>0.900089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024.0</th>\n",
       "      <td>0.905729</td>\n",
       "      <td>0.581585</td>\n",
       "      <td>0.903418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512.0</th>\n",
       "      <td>0.867639</td>\n",
       "      <td>0.552511</td>\n",
       "      <td>0.860665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Precision    Recall      NDCG\n",
       "Chunk Size                               \n",
       "2048.0       0.901042  0.609470  0.909941\n",
       "NaN          0.940774  0.604927  0.900089\n",
       "1024.0       0.905729  0.581585  0.903418\n",
       "512.0        0.867639  0.552511  0.860665"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.drop(columns=[eval_name, \"Chunk Overlap\", \"MAP\"]).groupby(\"Chunk Size\", dropna=False).mean().sort_values(by=\"Recall\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chunk Overlap</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.479302</td>\n",
       "      <td>0.979497</td>\n",
       "      <td>0.937772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200.0</th>\n",
       "      <td>0.478943</td>\n",
       "      <td>0.983321</td>\n",
       "      <td>0.934517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Precision    Recall      NDCG\n",
       "Chunk Overlap                               \n",
       "0.0             0.479302  0.979497  0.937772\n",
       "200.0           0.478943  0.983321  0.934517"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.drop(columns=[eval_name, \"Chunk Size\", \"MAP\"]).groupby(\"Chunk Overlap\").mean().sort_values(by=\"NDCG\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Generation (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "answer_correctness_system_prompt = \"\"\"You are a CORRECTNESS grader; providing the correctness of the given GENERATED ANSWER compared to the given GROUND TRUTH ANSWER.\n",
    "Respond only as a number from 0 to 10 where 0 is the least correct and 10 is the most correct.\n",
    "\n",
    "A few additional scoring guidelines:\n",
    "\n",
    "- Long GENERATED ANSWERS should score equally well as short GENERATED ANSWERS.\n",
    "\n",
    "- CORRECTNESS score should increase as the GENERATED ANSWER matches more accurately with the GROUND TRUTH ANSWER.\n",
    "\n",
    "- CORRECTNESS score should increase as the GENERATED ANSWER covers more parts of the GROUND TRUTH ANSWER accurately.\n",
    "\n",
    "- GENERATED ANSWERS that partially match the GROUND TRUTH ANSWER should score 2, 3, or 4. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS that mostly match the GROUND TRUTH ANSWER should get a score of 5, 6, 7, or 8. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS that fully match the GROUND TRUTH ANSWER should get a score of 9 or 10. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS must be fully accurate and comprehensive to the GROUND TRUTH ANSWER to get a score of 10.\n",
    "\n",
    "- Never elaborate.\"\"\"\n",
    "\n",
    "answer_correctness_user_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"GROUND TRUTH ANSWER: {ground_truth_answer}\n",
    "\n",
    "GENERATED ANSWER: {generated_answer}\n",
    "\n",
    "CORRECTNESS: \"\"\"\n",
    ")\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        \n",
    "for experiment_name, questions in datasets.items():\n",
    "    print(\"Evaluating\", experiment_name)\n",
    "    vector_stores[experiment_name].embeddings.show_progress_bar = False\n",
    "    retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": 10})\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | generator_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    for question_type, datasets in questions.items():\n",
    "        mean_answer_correctness = 0\n",
    "        for testset in datasets:\n",
    "            response = rag_chain.invoke(testset[\"question\"])\n",
    "            answer_correctness_prompt = answer_correctness_user_prompt.format(\n",
    "                ground_truth_answer=testset[\"ground_truth_answer\"], generated_answer=response\n",
    "            )\n",
    "\n",
    "            llm_messages = [\n",
    "                SystemMessage(content=answer_correctness_system_prompt),\n",
    "                HumanMessage(content=answer_correctness_prompt),\n",
    "            ]\n",
    "            response = make_request_with_backoff(llm_messages)\n",
    "\n",
    "            answer_correctness = re_0_10_rating(response.content)\n",
    "            mean_answer_correctness += answer_correctness\n",
    "        mean_answer_correctness /= len(datasets)\n",
    "        print(f\"Experiment: {experiment_name} Question Type: {question_type} Mean Answer Correctness: {mean_answer_correctness}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
