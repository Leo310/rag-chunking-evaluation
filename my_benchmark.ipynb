{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Evaluation Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/my_approach.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, TypedDict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_dotenv(override=True)\n",
    "\n",
    "data_dir = \"my_benchmark/\"\n",
    "os.environ['CHUNKING_BENCHMARK'] = data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and Save Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document is loaded as one Langchain document possibly to small to fit into a LLM. Therefore, we need to split these documents into smaller pieces of text for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import save_documents\n",
    "\n",
    "documents: List[Document] = []\n",
    "for file in os.listdir(data_dir+\"documents\"):\n",
    "    file_path = os.path.join(data_dir+\"documents\", file)\n",
    "    loader = TextLoader(file_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "save_documents(documents, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_documents\n",
    "documents = load_documents(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i chunking_strategies.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_chunks\n",
    "split_chunks: Dict[str, Document] = load_chunks(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ingest into vector store\n",
    "\n",
    "Using FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading markdown_header_recursive-1024-0\n",
      "Loading fixed_size-512-200\n",
      "Loading fixed_size-1024-0\n",
      "Loading markdown_header_recursive-512-200\n",
      "Loading semantic_chunks_90\n",
      "Loading fixed_size-1024-200\n",
      "Loading markdown_header\n",
      "Loading fixed_size-2048-0\n",
      "Loading markdown_header_recursive-2048-200\n",
      "Loading markdown_header_recursive-2048-0\n",
      "Loading markdown_header_recursive-1024-200\n",
      "Loading fixed_size-512-0\n",
      "Loading recursive-1024-200\n",
      "Loading markdown_header_parent\n",
      "Loading fixed_size-2048-200\n",
      "Loading recursive-2048-0\n",
      "Loading markdown_header_recursive-512-0\n",
      "Loading recursive-512-200\n",
      "Loading recursive-2048-200\n",
      "Loading semantic_chunks_recursive-95-2048-200\n",
      "Loading recursive-1024-0\n",
      "Loading semantic_chunks_95\n",
      "Loading recursive-512-0\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "vector_stores: Dict[str, VectorStore] = {}\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"Snowflake/snowflake-arctic-embed-l\",\n",
    "    # model_name=\"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "#     model_kwargs={\"device\": 0, 'trust_remote_code': True},  # Comment out to use CPU\n",
    "# )\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "model_name = (embeddings.model_name if hasattr(embeddings, 'model_name') else embeddings.model).replace(\"/\", \"_\")\n",
    "vector_store_dir = f\"{data_dir}vector_stores/{model_name}\"\n",
    "Path(vector_store_dir).mkdir(parents=True, exist_ok=True)\n",
    "for experiment_name, chunks in split_chunks.items():\n",
    "    if os.path.exists(f\"{vector_store_dir}/{experiment_name}\"):\n",
    "        print(\"Loading\", experiment_name)\n",
    "        vector_stores[experiment_name] = FAISS.load_local(f\"{vector_store_dir}/{experiment_name}\", embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        print(\"Indexing\", experiment_name)\n",
    "        vector_stores[experiment_name] = FAISS.from_documents(chunks, embeddings)\n",
    "        vector_stores[experiment_name].save_local(f\"{vector_store_dir}/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Golden Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Evaluation Golden Datasets for each Chunking Strategy should include the following:\n",
    "\n",
    "- Questions across Documents\n",
    "- Ground Truth Chunks (with graded Relevance)\n",
    "- Ground Truth Answers\n",
    "\n",
    "For Simple, Reasoning and Multi-Context Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import GoldenTestset\n",
    "\n",
    "class Questions(TypedDict):\n",
    "    simple: List[GoldenTestset]\n",
    "    reasoning: List[GoldenTestset]\n",
    "    multi_context: List[GoldenTestset]\n",
    "\n",
    "gold_dataset: Dict[str, Questions]  = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create golden dataset on subset of documents, to have some irrelevant documents left for some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_subset_sources = [data_dir+\"documents/sleep.md\", data_dir+\"documents/teeth.md\", data_dir+\"documents/time_management.md\", data_dir+\"documents/mentoring.md\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Generation with RAGAS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate synthetic Questions across Documents to challenge chunking strategies on multi-context queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "environ[\"RAGAS_DO_NOT_TRACK\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-3-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m generator \u001b[38;5;241m=\u001b[39m TestsetGenerator\u001b[38;5;241m.\u001b[39mfrom_langchain(generator_llm, critic_llm, embeddings)\n\u001b[1;32m     12\u001b[0m ragas_testset \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mgenerate_with_langchain_docs(\n\u001b[0;32m---> 13\u001b[0m     [document \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdocuments\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m document\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m documents_subset_sources],\n\u001b[1;32m     14\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     15\u001b[0m     distributions\u001b[38;5;241m=\u001b[39m{simple: \u001b[38;5;241m0.4\u001b[39m, reasoning: \u001b[38;5;241m0.4\u001b[39m, multi_context: \u001b[38;5;241m0.2\u001b[39m},\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m df \u001b[38;5;241m=\u001b[39m ragas_testset\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     18\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# ground truth contexts/chunks are determined in next step\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)\n",
    "\n",
    "ragas_testset = generator.generate_with_langchain_docs(\n",
    "    [document for document in documents if document.metadata[\"source\"] in documents_subset_sources],\n",
    "    test_size=10,\n",
    "    distributions={simple: 0.4, reasoning: 0.4, multi_context: 0.2},\n",
    ")\n",
    "df = ragas_testset.to_pandas()\n",
    "df = df.drop(columns=[\"contexts\"]) # ground truth contexts/chunks are determined in next step\n",
    "df.to_json(data_dir+\"ragas_testset.json\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_testset = pd.read_json(data_dir+\"ragas_testset.json\")\n",
    "for experiment_name in split_chunks.keys():\n",
    "    gold_dataset[experiment_name] = {\n",
    "        \"simple\": [],\n",
    "        \"reasoning\": [],\n",
    "        \"multi_context\": []\n",
    "    }\n",
    "    for _, row in ragas_testset.iterrows():\n",
    "        testset = {\n",
    "            \"question\": row['question'],\n",
    "            \"source\": [metadata[\"source\"] for metadata in row['metadata']],\n",
    "            \"ground_truth_chunks\": {},\n",
    "            \"ground_truth_answer\": row['ground_truth']\n",
    "        }\n",
    "        gold_dataset[experiment_name][row[\"evolution_type\"]].append(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Relevancy Score for each chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevancy Prompt is taken by Trulens. The difference is that I apply it to all chunks whereas Trulens only computed it on the retrieved chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for markdown-header-recursive-512-200\n",
      "Collecting ground truth for simple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:48<00:00, 12.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:02<00:00, 15.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for multi_context\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:23<00:00, 11.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for markdown-header-parent\n",
      "Collecting ground truth for simple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:14<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:17<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for multi_context\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from utils.llm_output_parser import re_0_10_rating\n",
    "\n",
    "system_prompt = \"\"\"You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\n",
    "    Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
    "\n",
    "    A few additional scoring guidelines:\n",
    "\n",
    "    - Long CONTEXTS should score equally well as short CONTEXTS.\n",
    "\n",
    "    - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\n",
    "\n",
    "    - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
    "\n",
    "    - Never elaborate.\"\"\"\n",
    "\n",
    "user_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"QUESTION: {question}\n",
    "\n",
    "    CONTEXT: {context}\n",
    "    \n",
    "    RELEVANCE: \"\"\"\n",
    ")\n",
    "\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def make_request_with_backoff(messages, retries=8):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = critic_llm.invoke(messages)\n",
    "            return response\n",
    "        except openai.RateLimitError as e:\n",
    "            if i == retries - 1:\n",
    "                raise e\n",
    "            wait_time = 2**i\n",
    "            print(f\"Rate limited, waiting {wait_time} seconds\")\n",
    "            time.sleep(wait_time)\n",
    "        except openai.APIError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def process_chunk(chunk, testset):\n",
    "    if chunk.metadata[\"source\"] not in testset[\"source\"]:\n",
    "        return None, None\n",
    "\n",
    "    judge_chunk_relevancy_prompt = user_prompt.format(\n",
    "        question=testset[\"question\"], context=chunk.page_content\n",
    "    )\n",
    "\n",
    "    llm_messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=judge_chunk_relevancy_prompt),\n",
    "    ]\n",
    "    response = make_request_with_backoff(llm_messages)\n",
    "    chunk_relevancy = re_0_10_rating(response.content)\n",
    "    if chunk_relevancy != 0.0:\n",
    "        return str(chunk.metadata[\"id\"]), chunk_relevancy\n",
    "    return None, None\n",
    "\n",
    "for experiment_name, questions in gold_dataset.items():\n",
    "    print(\"Collecting ground truth for\", experiment_name)\n",
    "    for question_type, testsets in questions.items():\n",
    "        print(\"Collecting ground truth for\", question_type)\n",
    "        for testset in tqdm(testsets):\n",
    "            ground_truth = {}\n",
    "            with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "                future_to_chunk = {\n",
    "                    executor.submit(process_chunk, chunk, testset): chunk\n",
    "                    for chunk in split_chunks[experiment_name]\n",
    "                }\n",
    "                for future in as_completed(future_to_chunk):\n",
    "                    chunk_id, relevancy = future.result()\n",
    "                    if chunk_id and relevancy:\n",
    "                        ground_truth[chunk_id] = relevancy\n",
    "            \n",
    "            if len(ground_truth):\n",
    "                testset[\"ground_truth_chunks\"] = ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Evaluation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir+'gold_dataset_1.json', 'w') as jsonl_file:\n",
    "    json.dump(gold_dataset, jsonl_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Evaluation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_dataset = {}\n",
    "with open(data_dir+'gold_dataset.json', 'r') as jsonl_file:\n",
    "    gold_dataset = json.load(jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import calculate_metrics, calculate_mean_metrics, EvalApproach\n",
    "\n",
    "SEL_APPROACH: EvalApproach = EvalApproach.FIXED_K\n",
    "FIXED_K = 5\n",
    "TOKEN_LIMIT = 3000\n",
    "\n",
    "results_list = []\n",
    "for experiment_name, questions in gold_dataset.items():\n",
    "    if experiment_name not in vector_stores:\n",
    "        continue\n",
    "\n",
    "    for question_type, testsets in questions.items():\n",
    "        metrics = []\n",
    "        for testset in testsets:\n",
    "            question = testset[\"question\"]\n",
    "            ground_truth = testset[\"ground_truth_chunks\"]\n",
    "            K = FIXED_K if SEL_APPROACH == EvalApproach.FIXED_K else 0\n",
    "            K = len(ground_truth) if SEL_APPROACH == EvalApproach.GROUND_TRUTH_K else K\n",
    "            K = 100 if SEL_APPROACH == EvalApproach.TOKEN_LIMIT else K # large number to ensure TOKEN_LIMIT is always reached\n",
    "\n",
    "            retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": K})\n",
    "            retrieved_chunks = retriever.invoke(question)\n",
    "\n",
    "            if SEL_APPROACH == EvalApproach.TOKEN_LIMIT:\n",
    "                # cap the number of retrieved chunks where sum of page_contents are below a fixed context window\n",
    "                retrieved_chunks_capped = []\n",
    "                total_context_length = 0\n",
    "                for chunk in retrieved_chunks:\n",
    "                    total_context_length += len(chunk.page_content)\n",
    "                    if total_context_length > TOKEN_LIMIT * 4: # as one token on average is approximately 4 characters\n",
    "                        break\n",
    "                    retrieved_chunks_capped.append(chunk)\n",
    "                \n",
    "                retrieved_chunks = retrieved_chunks_capped\n",
    "            \n",
    "            retrieved_chunk_ids = [\n",
    "                str(doc.metadata[\"id\"]) for doc in retrieved_chunks\n",
    "            ]\n",
    "            metrics.append(\n",
    "                calculate_metrics(\n",
    "                    retrieved_chunk_ids,\n",
    "                    ground_truth_chunks=list(ground_truth.keys()),\n",
    "                    ground_truth_relevancies=list(ground_truth.values()),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mean_metrics = calculate_mean_metrics(metrics)\n",
    "\n",
    "        try:\n",
    "            experiment_chunk_size = int(experiment_name.split(\"-\")[-2])\n",
    "            experiment_chunk_overlap = int(experiment_name.split(\"-\")[-1])\n",
    "        except:\n",
    "            experiment_chunk_size = None\n",
    "            experiment_chunk_overlap = None\n",
    "\n",
    "        results_list.append(\n",
    "            [\n",
    "                experiment_name.split(\"-\")[0],\n",
    "                experiment_chunk_size,\n",
    "                experiment_chunk_overlap,\n",
    "                question_type,\n",
    "                mean_metrics[\"precision\"],\n",
    "                mean_metrics[\"recall\"],\n",
    "                mean_metrics[\"map\"],\n",
    "                mean_metrics[\"ndcg\"],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "eval_name = f\"{SEL_APPROACH}-{FIXED_K}-{model_name}\" if SEL_APPROACH == EvalApproach.FIXED_K else \"\"\n",
    "eval_name = f\"{SEL_APPROACH}-{model_name}\" if SEL_APPROACH == EvalApproach.GROUND_TRUTH_K else eval_name\n",
    "eval_name = f\"{SEL_APPROACH}-{TOKEN_LIMIT}-{model_name}\" if SEL_APPROACH == EvalApproach.TOKEN_LIMIT else eval_name\n",
    "results = pd.DataFrame(\n",
    "    results_list,\n",
    "    columns=[\n",
    "        eval_name,\n",
    "        \"Chunk Size\",\n",
    "        \"Chunk Overlap\",\n",
    "        \"Question Type\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"MAP\",\n",
    "        \"NDCG\",\n",
    "    ],\n",
    ")\n",
    "results.to_csv(f\"{data_dir}results/{eval_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best average strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEL_APPROACH: EvalApproach = EvalApproach.FIXED_K\n",
    "FIXED_K = 20\n",
    "TOKEN_LIMIT = 3000\n",
    "model_name = \"text-embedding-3-small\"\n",
    "# model_name = \"Alibaba-NLP_gte-large-en-v1.5\"\n",
    "eval_name = f\"{SEL_APPROACH}-{FIXED_K}-{model_name}\" if SEL_APPROACH == EvalApproach.FIXED_K else \"\"\n",
    "eval_name = f\"{SEL_APPROACH}-{model_name}\" if SEL_APPROACH == EvalApproach.GROUND_TRUTH_K else eval_name\n",
    "eval_name = f\"{SEL_APPROACH}-{TOKEN_LIMIT}-{model_name}\" if SEL_APPROACH == EvalApproach.TOKEN_LIMIT else eval_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fixed-K-20-text-embedding-3-small</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>semantic_chunks_95</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic_chunks_recursive</th>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.941154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header_parent</th>\n",
       "      <td>0.595833</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.962851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic_chunks_90</th>\n",
       "      <td>0.570833</td>\n",
       "      <td>0.957816</td>\n",
       "      <td>0.935623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.935702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_size</th>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.824929</td>\n",
       "      <td>0.929322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recursive</th>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.781641</td>\n",
       "      <td>0.918503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header_recursive</th>\n",
       "      <td>0.821528</td>\n",
       "      <td>0.757926</td>\n",
       "      <td>0.915479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Precision    Recall      NDCG\n",
       "Fixed-K-20-text-embedding-3-small                               \n",
       "semantic_chunks_95                  0.375000  1.000000  0.967677\n",
       "semantic_chunks_recursive           0.616667  0.966667  0.941154\n",
       "markdown_header_parent              0.595833  0.960000  0.962851\n",
       "semantic_chunks_90                  0.570833  0.957816  0.935623\n",
       "markdown_header                     0.583333  0.950000  0.935702\n",
       "fixed_size                          0.687500  0.824929  0.929322\n",
       "recursive                           0.745833  0.781641  0.918503\n",
       "markdown_header_recursive           0.821528  0.757926  0.915479"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(f\"{data_dir}results/{eval_name}.csv\")\n",
    "results_view = results.drop(columns=[\"Question Type\", \"Chunk Size\", \"Chunk Overlap\", \"MAP\"]).groupby(eval_name).mean().sort_values(by=\"Recall\", ascending=False)\n",
    "results_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token-Limit-6000-text-embedding-3-small</th>\n",
       "      <th>Chunk Size</th>\n",
       "      <th>Chunk Overlap</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">fixed_size</th>\n",
       "      <th>2048.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.598825</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.949561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.603901</td>\n",
       "      <td>0.967262</td>\n",
       "      <td>0.944149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic_chunks_recursive</th>\n",
       "      <th>2048.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.607676</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.939740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">recursive</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2048.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.660256</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>0.949291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200.0</th>\n",
       "      <td>0.647436</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>0.957929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_size</th>\n",
       "      <th>512.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.592163</td>\n",
       "      <td>0.960606</td>\n",
       "      <td>0.930967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header_recursive</th>\n",
       "      <th>1024.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.575662</td>\n",
       "      <td>0.953175</td>\n",
       "      <td>0.934840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">recursive</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1024.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.646141</td>\n",
       "      <td>0.952703</td>\n",
       "      <td>0.917986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.639163</td>\n",
       "      <td>0.952050</td>\n",
       "      <td>0.922390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic_chunks_95</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>0.645412</td>\n",
       "      <td>0.951389</td>\n",
       "      <td>0.965582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">markdown_header_recursive</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2048.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.571824</td>\n",
       "      <td>0.948523</td>\n",
       "      <td>0.920172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200.0</th>\n",
       "      <td>0.595544</td>\n",
       "      <td>0.948523</td>\n",
       "      <td>0.924071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_size</th>\n",
       "      <th>2048.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.642094</td>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.949244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>0.579502</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.939138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header_recursive</th>\n",
       "      <th>512.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.576619</td>\n",
       "      <td>0.944401</td>\n",
       "      <td>0.932416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_size</th>\n",
       "      <th>1024.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.711353</td>\n",
       "      <td>0.943627</td>\n",
       "      <td>0.957541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header_parent</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>0.609989</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.962145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header_recursive</th>\n",
       "      <th>1024.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.645454</td>\n",
       "      <td>0.941168</td>\n",
       "      <td>0.950046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">recursive</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">512.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.605181</td>\n",
       "      <td>0.938188</td>\n",
       "      <td>0.924739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200.0</th>\n",
       "      <td>0.658045</td>\n",
       "      <td>0.934907</td>\n",
       "      <td>0.936084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic_chunks_90</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>0.551263</td>\n",
       "      <td>0.932453</td>\n",
       "      <td>0.937475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown_header_recursive</th>\n",
       "      <th>512.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.753867</td>\n",
       "      <td>0.931591</td>\n",
       "      <td>0.937313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed_size</th>\n",
       "      <th>512.0</th>\n",
       "      <th>200.0</th>\n",
       "      <td>0.825702</td>\n",
       "      <td>0.866454</td>\n",
       "      <td>0.922046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  Precision  \\\n",
       "Token-Limit-6000-text-embedding-3-small Chunk Size Chunk Overlap              \n",
       "fixed_size                              2048.0     0.0             0.598825   \n",
       "                                        1024.0     0.0             0.603901   \n",
       "semantic_chunks_recursive               2048.0     200.0           0.607676   \n",
       "recursive                               2048.0     0.0             0.660256   \n",
       "                                                   200.0           0.647436   \n",
       "fixed_size                              512.0      0.0             0.592163   \n",
       "markdown_header_recursive               1024.0     0.0             0.575662   \n",
       "recursive                               1024.0     200.0           0.646141   \n",
       "                                                   0.0             0.639163   \n",
       "semantic_chunks_95                      NaN        NaN             0.645412   \n",
       "markdown_header_recursive               2048.0     0.0             0.571824   \n",
       "                                                   200.0           0.595544   \n",
       "fixed_size                              2048.0     200.0           0.642094   \n",
       "markdown_header                         NaN        NaN             0.579502   \n",
       "markdown_header_recursive               512.0      0.0             0.576619   \n",
       "fixed_size                              1024.0     200.0           0.711353   \n",
       "markdown_header_parent                  NaN        NaN             0.609989   \n",
       "markdown_header_recursive               1024.0     200.0           0.645454   \n",
       "recursive                               512.0      0.0             0.605181   \n",
       "                                                   200.0           0.658045   \n",
       "semantic_chunks_90                      NaN        NaN             0.551263   \n",
       "markdown_header_recursive               512.0      200.0           0.753867   \n",
       "fixed_size                              512.0      200.0           0.825702   \n",
       "\n",
       "                                                                    Recall  \\\n",
       "Token-Limit-6000-text-embedding-3-small Chunk Size Chunk Overlap             \n",
       "fixed_size                              2048.0     0.0            0.976190   \n",
       "                                        1024.0     0.0            0.967262   \n",
       "semantic_chunks_recursive               2048.0     200.0          0.966667   \n",
       "recursive                               2048.0     0.0            0.963542   \n",
       "                                                   200.0          0.963542   \n",
       "fixed_size                              512.0      0.0            0.960606   \n",
       "markdown_header_recursive               1024.0     0.0            0.953175   \n",
       "recursive                               1024.0     200.0          0.952703   \n",
       "                                                   0.0            0.952050   \n",
       "semantic_chunks_95                      NaN        NaN            0.951389   \n",
       "markdown_header_recursive               2048.0     0.0            0.948523   \n",
       "                                                   200.0          0.948523   \n",
       "fixed_size                              2048.0     200.0          0.947917   \n",
       "markdown_header                         NaN        NaN            0.946667   \n",
       "markdown_header_recursive               512.0      0.0            0.944401   \n",
       "fixed_size                              1024.0     200.0          0.943627   \n",
       "markdown_header_parent                  NaN        NaN            0.943333   \n",
       "markdown_header_recursive               1024.0     200.0          0.941168   \n",
       "recursive                               512.0      0.0            0.938188   \n",
       "                                                   200.0          0.934907   \n",
       "semantic_chunks_90                      NaN        NaN            0.932453   \n",
       "markdown_header_recursive               512.0      200.0          0.931591   \n",
       "fixed_size                              512.0      200.0          0.866454   \n",
       "\n",
       "                                                                      NDCG  \n",
       "Token-Limit-6000-text-embedding-3-small Chunk Size Chunk Overlap            \n",
       "fixed_size                              2048.0     0.0            0.949561  \n",
       "                                        1024.0     0.0            0.944149  \n",
       "semantic_chunks_recursive               2048.0     200.0          0.939740  \n",
       "recursive                               2048.0     0.0            0.949291  \n",
       "                                                   200.0          0.957929  \n",
       "fixed_size                              512.0      0.0            0.930967  \n",
       "markdown_header_recursive               1024.0     0.0            0.934840  \n",
       "recursive                               1024.0     200.0          0.917986  \n",
       "                                                   0.0            0.922390  \n",
       "semantic_chunks_95                      NaN        NaN            0.965582  \n",
       "markdown_header_recursive               2048.0     0.0            0.920172  \n",
       "                                                   200.0          0.924071  \n",
       "fixed_size                              2048.0     200.0          0.949244  \n",
       "markdown_header                         NaN        NaN            0.939138  \n",
       "markdown_header_recursive               512.0      0.0            0.932416  \n",
       "fixed_size                              1024.0     200.0          0.957541  \n",
       "markdown_header_parent                  NaN        NaN            0.962145  \n",
       "markdown_header_recursive               1024.0     200.0          0.950046  \n",
       "recursive                               512.0      0.0            0.924739  \n",
       "                                                   200.0          0.936084  \n",
       "semantic_chunks_90                      NaN        NaN            0.937475  \n",
       "markdown_header_recursive               512.0      200.0          0.937313  \n",
       "fixed_size                              512.0      200.0          0.922046  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(f\"{data_dir}results/{eval_name}.csv\")\n",
    "results_view = results.drop(columns=[\"Question Type\", \"MAP\"]).groupby([eval_name, \"Chunk Size\", \"Chunk Overlap\"], dropna=False).mean().sort_values(by=\"Recall\", ascending=False)\n",
    "results_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_view.to_csv(f\"{data_dir}results/{eval_name}_top_strategies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chunk Size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.596542</td>\n",
       "      <td>0.943461</td>\n",
       "      <td>0.951085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048.0</th>\n",
       "      <td>0.617665</td>\n",
       "      <td>0.959272</td>\n",
       "      <td>0.941430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024.0</th>\n",
       "      <td>0.636946</td>\n",
       "      <td>0.951664</td>\n",
       "      <td>0.937825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512.0</th>\n",
       "      <td>0.668596</td>\n",
       "      <td>0.929358</td>\n",
       "      <td>0.930594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Precision    Recall      NDCG\n",
       "Chunk Size                               \n",
       "NaN          0.596542  0.943461  0.951085\n",
       "2048.0       0.617665  0.959272  0.941430\n",
       "1024.0       0.636946  0.951664  0.937825\n",
       "512.0        0.668596  0.929358  0.930594"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(f\"{data_dir}results/{eval_name}.csv\")\n",
    "results_view = results.drop(columns=[eval_name,\"Question Type\", \"Chunk Overlap\", \"MAP\"]).groupby(\"Chunk Size\", dropna=False).mean().sort_values(by=\"NDCG\", ascending=False)\n",
    "results_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chunk Overlap</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200.0</th>\n",
       "      <td>0.673331</td>\n",
       "      <td>0.939710</td>\n",
       "      <td>0.939200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.602622</td>\n",
       "      <td>0.955993</td>\n",
       "      <td>0.934281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Precision    Recall      NDCG\n",
       "Chunk Overlap                               \n",
       "200.0           0.673331  0.939710  0.939200\n",
       "0.0             0.602622  0.955993  0.934281"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(f\"{data_dir}results/{eval_name}.csv\")\n",
    "results_view = results.drop(columns=[eval_name,\"Question Type\", \"Chunk Size\", \"MAP\"]).groupby(\"Chunk Overlap\").mean().sort_values(by=\"NDCG\", ascending=False)\n",
    "results_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "answer_correctness_system_prompt = \"\"\"You are a CORRECTNESS grader; providing the correctness of the given GENERATED ANSWER compared to the given GROUND TRUTH ANSWER.\n",
    "Respond only as a number from 0 to 10 where 0 is the least correct and 10 is the most correct.\n",
    "\n",
    "A few additional scoring guidelines:\n",
    "\n",
    "- Long GENERATED ANSWERS should score equally well as short GENERATED ANSWERS.\n",
    "\n",
    "- CORRECTNESS score should increase as the GENERATED ANSWER matches more accurately with the GROUND TRUTH ANSWER.\n",
    "\n",
    "- CORRECTNESS score should increase as the GENERATED ANSWER covers more parts of the GROUND TRUTH ANSWER accurately.\n",
    "\n",
    "- GENERATED ANSWERS that partially match the GROUND TRUTH ANSWER should score 2, 3, or 4. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS that mostly match the GROUND TRUTH ANSWER should get a score of 5, 6, 7, or 8. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS that fully match the GROUND TRUTH ANSWER should get a score of 9 or 10. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS must be fully accurate and comprehensive to the GROUND TRUTH ANSWER to get a score of 10.\n",
    "\n",
    "- Never elaborate.\"\"\"\n",
    "\n",
    "answer_correctness_user_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"GROUND TRUTH ANSWER: {ground_truth_answer}\n",
    "\n",
    "GENERATED ANSWER: {generated_answer}\n",
    "\n",
    "CORRECTNESS: \"\"\"\n",
    ")\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        \n",
    "for experiment_name, questions in gold_dataset.items():\n",
    "    print(\"Evaluating\", experiment_name)\n",
    "    vector_stores[experiment_name].embeddings.show_progress_bar = False\n",
    "    retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": 10})\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | generator_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    for question_type, testsets in questions.items():\n",
    "        mean_answer_correctness = 0\n",
    "        for testset in testsets:\n",
    "            response = rag_chain.invoke(testset[\"question\"])\n",
    "            answer_correctness_prompt = answer_correctness_user_prompt.format(\n",
    "                ground_truth_answer=testset[\"ground_truth_answer\"], generated_answer=response\n",
    "            )\n",
    "\n",
    "            llm_messages = [\n",
    "                SystemMessage(content=answer_correctness_system_prompt),\n",
    "                HumanMessage(content=answer_correctness_prompt),\n",
    "            ]\n",
    "            response = make_request_with_backoff(llm_messages)\n",
    "\n",
    "            answer_correctness = re_0_10_rating(response.content)\n",
    "            mean_answer_correctness += answer_correctness\n",
    "        mean_answer_correctness /= len(testsets)\n",
    "        print(f\"Experiment: {experiment_name} Question Type: {question_type} Mean Answer Correctness: {mean_answer_correctness}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
