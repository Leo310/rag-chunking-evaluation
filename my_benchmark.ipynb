{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Evaluation Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/my_approach.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, TypedDict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_dotenv(override=True)\n",
    "\n",
    "data_dir = \"my_benchmark/\"\n",
    "os.environ['CHUNKING_BENCHMARK'] = data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and Save Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document is loaded as one Langchain document possibly to small to fit into a LLM. Therefore, we need to split these documents into smaller pieces of text for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import save_documents\n",
    "\n",
    "documents: List[Document] = []\n",
    "for file in os.listdir(data_dir+\"documents\"):\n",
    "    file_path = os.path.join(data_dir+\"documents\", file)\n",
    "    loader = TextLoader(file_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "save_documents(documents, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_documents\n",
    "documents = load_documents(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i chunking_strategies.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_chunks\n",
    "split_chunks: Dict[str, Document] = load_chunks(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ingest into vector store\n",
    "\n",
    "Using FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing markdown-header-recursive-512-0\n",
      "Indexing fixed-size-2048-0\n",
      "Indexing markdown-header-recursive-1024-0\n",
      "Indexing markdown-header-recursive-2048-200\n",
      "Indexing recursive-1024-200\n",
      "Indexing fixed-size-512-0\n",
      "Indexing fixed-size-1024-0\n",
      "Indexing markdown-header-recursive-1024-200\n",
      "Indexing markdown-header-recursive-2048-0\n",
      "Indexing fixed-size-1024-200\n",
      "Indexing markdown-header\n",
      "Indexing recursive-2048-0\n",
      "Indexing fixed-size-2048-200\n",
      "Indexing recursive-512-200\n",
      "Indexing recursive-2048-200\n",
      "Indexing recursive-1024-0\n",
      "Indexing markdown-header-parent\n",
      "Indexing fixed-size-512-200\n",
      "Indexing semantic-chunks-95-recursive-2048-200\n",
      "Indexing semantic-chunks-90\n",
      "Indexing semantic-chunks-95\n",
      "Indexing recursive-512-0\n",
      "Indexing markdown-header-recursive-512-200\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "vector_stores: Dict[str, VectorStore] = {}\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"Snowflake/snowflake-arctic-embed-l\",\n",
    "#     model_kwargs={\"device\": 0, 'trust_remote_code': True},  # Comment out to use CPU\n",
    "# )\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store_dir = f\"{data_dir}vector_stores/{embeddings.model.replace('/', '-')}\"\n",
    "Path(vector_store_dir).mkdir(parents=True, exist_ok=True)\n",
    "for experiment_name, chunks in split_chunks.items():\n",
    "    if os.path.exists(f\"{vector_store_dir}/{experiment_name}\"):\n",
    "        print(\"Loading\", experiment_name)\n",
    "        vector_stores[experiment_name] = FAISS.load_local(f\"{vector_store_dir}/{experiment_name}\", embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        print(\"Indexing\", experiment_name)\n",
    "        vector_stores[experiment_name] = FAISS.from_documents(chunks, embeddings)\n",
    "        vector_stores[experiment_name].save_local(f\"{vector_store_dir}/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Golden Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Evaluation Golden Datasets for each Chunking Strategy should include the following:\n",
    "\n",
    "- Questions across Documents\n",
    "- Ground Truth Chunks (with graded Relevance)\n",
    "- Ground Truth Answers\n",
    "\n",
    "For Simple, Reasoning and Multi-Context Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import GoldenTestset\n",
    "\n",
    "class Questions(TypedDict):\n",
    "    simple: List[GoldenTestset]\n",
    "    reasoning: List[GoldenTestset]\n",
    "    multi_context: List[GoldenTestset]\n",
    "\n",
    "gold_dataset: Dict[str, Questions]  = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create golden dataset on subset of documents, to have some irrelevant documents left for some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_subset_sources = [data_dir+\"documents/sleep.md\", data_dir+\"documents/teeth.md\", data_dir+\"documents/time_management.md\", data_dir+\"documents/mentoring.md\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Generation with RAGAS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate synthetic Questions across Documents to challenge chunking strategies on multi-context queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "environ[\"RAGAS_DO_NOT_TRACK\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83488d2319fb471283241d778b913912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620b257bf4ca47d2af6d5db7c02ca6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)\n",
    "\n",
    "ragas_testset = generator.generate_with_langchain_docs(\n",
    "    [document for document in documents if document.metadata[\"source\"] in documents_subset_sources],\n",
    "    test_size=10,\n",
    "    distributions={simple: 0.4, reasoning: 0.4, multi_context: 0.2},\n",
    ")\n",
    "df = ragas_testset.to_pandas()\n",
    "df = df.drop(columns=[\"contexts\"]) # ground truth contexts/chunks are determined in next step\n",
    "df.to_csv(data_dir+\"ragas_testset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping fixed-size-2048-0\n",
      "Skipping recursive-1024-200\n",
      "Skipping fixed-size-512-0\n",
      "Skipping fixed-size-1024-0\n",
      "Skipping fixed-size-1024-200\n",
      "Skipping recursive-2048-0\n",
      "Skipping fixed-size-2048-200\n",
      "Skipping recursive-512-200\n",
      "Skipping recursive-2048-200\n",
      "Skipping recursive-1024-0\n",
      "Skipping fixed-size-512-200\n",
      "Skipping semantic-chunks-95-recursive-2048-200\n",
      "Skipping semantic-chunks-90\n",
      "Skipping semantic-chunks-95\n",
      "Skipping recursive-512-0\n"
     ]
    }
   ],
   "source": [
    "ragas_testset = pd.read_csv(data_dir+\"ragas_testset.csv\")\n",
    "for experiment_name in split_chunks.keys():\n",
    "    if \"markdown-header\" not in experiment_name:\n",
    "        continue\n",
    "    gold_dataset[experiment_name] = {\n",
    "        \"simple\": [],\n",
    "        \"reasoning\": [],\n",
    "        \"multi_context\": []\n",
    "    }\n",
    "    for _, row in ragas_testset.iterrows():\n",
    "        testset = {\n",
    "            \"question\": row['question'],\n",
    "            \"source\": row['metadata'],\n",
    "            \"ground_truth_chunks\": {},\n",
    "            \"ground_truth_answer\": row['ground_truth']\n",
    "        }\n",
    "        gold_dataset[experiment_name][row[\"evolution_type\"]].append(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Relevancy Score for each chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevancy Prompt is taken by Trulens. The difference is that I apply it to all chunks whereas Trulens only computed it on the retrieved chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for markdown-header-recursive-512-200\n",
      "Collecting ground truth for simple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:48<00:00, 12.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:02<00:00, 15.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for multi_context\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:23<00:00, 11.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for markdown-header-parent\n",
      "Collecting ground truth for simple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:14<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:17<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ground truth for multi_context\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from utils.llm_output_parser import re_0_10_rating\n",
    "\n",
    "system_prompt = \"\"\"You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\n",
    "    Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
    "\n",
    "    A few additional scoring guidelines:\n",
    "\n",
    "    - Long CONTEXTS should score equally well as short CONTEXTS.\n",
    "\n",
    "    - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\n",
    "\n",
    "    - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
    "\n",
    "    - Never elaborate.\"\"\"\n",
    "\n",
    "user_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"QUESTION: {question}\n",
    "\n",
    "    CONTEXT: {context}\n",
    "    \n",
    "    RELEVANCE: \"\"\"\n",
    ")\n",
    "\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def make_request_with_backoff(messages, retries=8):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = critic_llm.invoke(messages)\n",
    "            return response\n",
    "        except openai.RateLimitError as e:\n",
    "            if i == retries - 1:\n",
    "                raise e\n",
    "            wait_time = 2**i\n",
    "            print(f\"Rate limited, waiting {wait_time} seconds\")\n",
    "            time.sleep(wait_time)\n",
    "        except openai.APIError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def process_chunk(chunk, testset):\n",
    "    if chunk.metadata[\"source\"] not in testset[\"source\"]:\n",
    "        return None, None\n",
    "\n",
    "    judge_chunk_relevancy_prompt = user_prompt.format(\n",
    "        question=testset[\"question\"], context=chunk.page_content\n",
    "    )\n",
    "\n",
    "    llm_messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=judge_chunk_relevancy_prompt),\n",
    "    ]\n",
    "    response = make_request_with_backoff(llm_messages)\n",
    "    chunk_relevancy = re_0_10_rating(response.content)\n",
    "    if chunk_relevancy != 0.0:\n",
    "        return str(chunk.metadata[\"id\"]), chunk_relevancy\n",
    "    return None, None\n",
    "\n",
    "for experiment_name, questions in gold_dataset.items():\n",
    "    if \"markdown-header-recursive-512-200\" not in experiment_name and \"markdown-header-parent\" not in experiment_name:\n",
    "        continue\n",
    "    print(\"Collecting ground truth for\", experiment_name)\n",
    "    for question_type, testsets in questions.items():\n",
    "        print(\"Collecting ground truth for\", question_type)\n",
    "        for testset in tqdm(testsets):\n",
    "            ground_truth = {}\n",
    "            with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "                future_to_chunk = {\n",
    "                    executor.submit(process_chunk, chunk, testset): chunk\n",
    "                    for chunk in split_chunks[experiment_name]\n",
    "                }\n",
    "                for future in as_completed(future_to_chunk):\n",
    "                    chunk_id, relevancy = future.result()\n",
    "                    if chunk_id and relevancy:\n",
    "                        ground_truth[chunk_id] = relevancy\n",
    "            \n",
    "            if len(ground_truth):\n",
    "                testset[\"ground_truth_chunks\"] = ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Evaluation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir+'gold_dataset_1.json', 'w') as jsonl_file:\n",
    "    json.dump(gold_dataset, jsonl_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Evaluation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_dataset = {}\n",
    "with open(data_dir+'gold_dataset.json', 'r') as jsonl_file:\n",
    "    gold_dataset = json.load(jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import calculate_metrics, calculate_mean_metrics\n",
    "\n",
    "K = None\n",
    "results = pd.DataFrame(columns=[f\"Experiment@{K or 'Dyn'}\", \"Question Type\", \"Precision\", \"Recall\", \"MAP\", \"NDCG\"])\n",
    "\n",
    "for experiment_name, questions in gold_dataset.items():\n",
    "    if experiment_name not in vector_stores:\n",
    "        continue\n",
    "    \n",
    "    for question_type, testsets in questions.items():\n",
    "        metrics = []\n",
    "        for testset in testsets:\n",
    "            question = testset[\"question\"]\n",
    "            ground_truth = testset[\"ground_truth_chunks\"]\n",
    "            retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": K or len(ground_truth)})\n",
    "            retrieved_chunks = retriever.invoke(question)\n",
    "            retrieved_chunk_ids = [str(doc.metadata[\"id\"]) for doc in retrieved_chunks]\n",
    "            metrics.append(calculate_metrics(retrieved_chunk_ids, ground_truth_chunks=list(ground_truth.keys()), ground_truth_relevancies=list(ground_truth.values()), K=K or len(ground_truth)))\n",
    "        \n",
    "        mean_metrics = calculate_mean_metrics(metrics)\n",
    "        \n",
    "        results.loc[len(results)] = [\n",
    "            experiment_name,\n",
    "            question_type,\n",
    "            mean_metrics[\"precision\"],\n",
    "            mean_metrics[\"recall\"],\n",
    "            mean_metrics[\"map\"],\n",
    "            mean_metrics[\"ndcg\"]\n",
    "        ]\n",
    "\n",
    "results.to_csv(f\"{data_dir}results/{K or 'Dyn'}-k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>MAP</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment@Dyn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recursive-2048-0</th>\n",
       "      <td>0.937616</td>\n",
       "      <td>0.937616</td>\n",
       "      <td>0.927859</td>\n",
       "      <td>0.962165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed-size-2048-0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recursive-2048-200</th>\n",
       "      <td>0.914881</td>\n",
       "      <td>0.914881</td>\n",
       "      <td>0.911222</td>\n",
       "      <td>0.953246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed-size-2048-200</th>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.944813</td>\n",
       "      <td>0.945893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown-header-parent</th>\n",
       "      <td>0.896152</td>\n",
       "      <td>0.896152</td>\n",
       "      <td>0.882449</td>\n",
       "      <td>0.945647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed-size-1024-0</th>\n",
       "      <td>0.908189</td>\n",
       "      <td>0.908189</td>\n",
       "      <td>0.899410</td>\n",
       "      <td>0.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed-size-1024-200</th>\n",
       "      <td>0.907985</td>\n",
       "      <td>0.907985</td>\n",
       "      <td>0.898566</td>\n",
       "      <td>0.943372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown-header</th>\n",
       "      <td>0.902953</td>\n",
       "      <td>0.902953</td>\n",
       "      <td>0.888976</td>\n",
       "      <td>0.936472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown-header-recursive-1024-0</th>\n",
       "      <td>0.898361</td>\n",
       "      <td>0.898361</td>\n",
       "      <td>0.877940</td>\n",
       "      <td>0.934421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recursive-1024-200</th>\n",
       "      <td>0.909035</td>\n",
       "      <td>0.909035</td>\n",
       "      <td>0.891915</td>\n",
       "      <td>0.933424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown-header-recursive-2048-200</th>\n",
       "      <td>0.896120</td>\n",
       "      <td>0.896120</td>\n",
       "      <td>0.879313</td>\n",
       "      <td>0.932515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown-header-recursive-2048-0</th>\n",
       "      <td>0.898760</td>\n",
       "      <td>0.898760</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.931919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown-header-recursive-1024-200</th>\n",
       "      <td>0.885343</td>\n",
       "      <td>0.885343</td>\n",
       "      <td>0.866621</td>\n",
       "      <td>0.931475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic-chunks-95-recursive-2048-200</th>\n",
       "      <td>0.883280</td>\n",
       "      <td>0.883280</td>\n",
       "      <td>0.872280</td>\n",
       "      <td>0.923716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown-header-recursive-512-200</th>\n",
       "      <td>0.890410</td>\n",
       "      <td>0.890410</td>\n",
       "      <td>0.862929</td>\n",
       "      <td>0.921870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recursive-1024-0</th>\n",
       "      <td>0.908582</td>\n",
       "      <td>0.908582</td>\n",
       "      <td>0.883503</td>\n",
       "      <td>0.921535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recursive-512-200</th>\n",
       "      <td>0.888946</td>\n",
       "      <td>0.888946</td>\n",
       "      <td>0.855905</td>\n",
       "      <td>0.918543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic-chunks-90</th>\n",
       "      <td>0.842598</td>\n",
       "      <td>0.842598</td>\n",
       "      <td>0.827627</td>\n",
       "      <td>0.918259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recursive-512-0</th>\n",
       "      <td>0.891057</td>\n",
       "      <td>0.891057</td>\n",
       "      <td>0.856826</td>\n",
       "      <td>0.918118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic-chunks-95</th>\n",
       "      <td>0.918651</td>\n",
       "      <td>0.918651</td>\n",
       "      <td>0.889503</td>\n",
       "      <td>0.915755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markdown-header-recursive-512-0</th>\n",
       "      <td>0.875401</td>\n",
       "      <td>0.875401</td>\n",
       "      <td>0.852554</td>\n",
       "      <td>0.915089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed-size-512-0</th>\n",
       "      <td>0.880589</td>\n",
       "      <td>0.880589</td>\n",
       "      <td>0.864030</td>\n",
       "      <td>0.905804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed-size-512-200</th>\n",
       "      <td>0.869101</td>\n",
       "      <td>0.869101</td>\n",
       "      <td>0.836235</td>\n",
       "      <td>0.903659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Precision    Recall       MAP      NDCG\n",
       "Experiment@Dyn                                                                \n",
       "recursive-2048-0                        0.937616  0.937616  0.927859  0.962165\n",
       "fixed-size-2048-0                       1.000000  1.000000  1.000000  0.960288\n",
       "recursive-2048-200                      0.914881  0.914881  0.911222  0.953246\n",
       "fixed-size-2048-200                     0.945139  0.945139  0.944813  0.945893\n",
       "markdown-header-parent                  0.896152  0.896152  0.882449  0.945647\n",
       "fixed-size-1024-0                       0.908189  0.908189  0.899410  0.943800\n",
       "fixed-size-1024-200                     0.907985  0.907985  0.898566  0.943372\n",
       "markdown-header                         0.902953  0.902953  0.888976  0.936472\n",
       "markdown-header-recursive-1024-0        0.898361  0.898361  0.877940  0.934421\n",
       "recursive-1024-200                      0.909035  0.909035  0.891915  0.933424\n",
       "markdown-header-recursive-2048-200      0.896120  0.896120  0.879313  0.932515\n",
       "markdown-header-recursive-2048-0        0.898760  0.898760  0.880109  0.931919\n",
       "markdown-header-recursive-1024-200      0.885343  0.885343  0.866621  0.931475\n",
       "semantic-chunks-95-recursive-2048-200   0.883280  0.883280  0.872280  0.923716\n",
       "markdown-header-recursive-512-200       0.890410  0.890410  0.862929  0.921870\n",
       "recursive-1024-0                        0.908582  0.908582  0.883503  0.921535\n",
       "recursive-512-200                       0.888946  0.888946  0.855905  0.918543\n",
       "semantic-chunks-90                      0.842598  0.842598  0.827627  0.918259\n",
       "recursive-512-0                         0.891057  0.891057  0.856826  0.918118\n",
       "semantic-chunks-95                      0.918651  0.918651  0.889503  0.915755\n",
       "markdown-header-recursive-512-0         0.875401  0.875401  0.852554  0.915089\n",
       "fixed-size-512-0                        0.880589  0.880589  0.864030  0.905804\n",
       "fixed-size-512-200                      0.869101  0.869101  0.836235  0.903659"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = None\n",
    "results = pd.read_csv(f\"{data_dir}results/{K or 'Dyn'}-k.csv\")\n",
    "results_view = results.drop(columns=[\"Question Type\"]).groupby(f\"Experiment@{K or 'Dyn'}\").mean().sort_values(by=\"NDCG\", ascending=False)\n",
    "results_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "answer_correctness_system_prompt = \"\"\"You are a CORRECTNESS grader; providing the correctness of the given GENERATED ANSWER compared to the given GROUND TRUTH ANSWER.\n",
    "Respond only as a number from 0 to 10 where 0 is the least correct and 10 is the most correct.\n",
    "\n",
    "A few additional scoring guidelines:\n",
    "\n",
    "- Long GENERATED ANSWERS should score equally well as short GENERATED ANSWERS.\n",
    "\n",
    "- CORRECTNESS score should increase as the GENERATED ANSWER matches more accurately with the GROUND TRUTH ANSWER.\n",
    "\n",
    "- CORRECTNESS score should increase as the GENERATED ANSWER covers more parts of the GROUND TRUTH ANSWER accurately.\n",
    "\n",
    "- GENERATED ANSWERS that partially match the GROUND TRUTH ANSWER should score 2, 3, or 4. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS that mostly match the GROUND TRUTH ANSWER should get a score of 5, 6, 7, or 8. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS that fully match the GROUND TRUTH ANSWER should get a score of 9 or 10. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS must be fully accurate and comprehensive to the GROUND TRUTH ANSWER to get a score of 10.\n",
    "\n",
    "- Never elaborate.\"\"\"\n",
    "\n",
    "answer_correctness_user_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"GROUND TRUTH ANSWER: {ground_truth_answer}\n",
    "\n",
    "GENERATED ANSWER: {generated_answer}\n",
    "\n",
    "CORRECTNESS: \"\"\"\n",
    ")\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        \n",
    "for experiment_name, questions in gold_dataset.items():\n",
    "    print(\"Evaluating\", experiment_name)\n",
    "    vector_stores[experiment_name].embeddings.show_progress_bar = False\n",
    "    retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": 10})\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | generator_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    for question_type, testsets in questions.items():\n",
    "        mean_answer_correctness = 0\n",
    "        for testset in testsets:\n",
    "            response = rag_chain.invoke(testset[\"question\"])\n",
    "            answer_correctness_prompt = answer_correctness_user_prompt.format(\n",
    "                ground_truth_answer=testset[\"ground_truth_answer\"], generated_answer=response\n",
    "            )\n",
    "\n",
    "            llm_messages = [\n",
    "                SystemMessage(content=answer_correctness_system_prompt),\n",
    "                HumanMessage(content=answer_correctness_prompt),\n",
    "            ]\n",
    "            response = make_request_with_backoff(llm_messages)\n",
    "\n",
    "            answer_correctness = re_0_10_rating(response.content)\n",
    "            mean_answer_correctness += answer_correctness\n",
    "        mean_answer_correctness /= len(testsets)\n",
    "        print(f\"Experiment: {experiment_name} Question Type: {question_type} Mean Answer Correctness: {mean_answer_correctness}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
