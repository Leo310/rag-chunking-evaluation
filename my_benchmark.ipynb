{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# My Evaluation Approach\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](assets/my_approach.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Document Preparation**: Load the entire document and extract content.\n",
        "2. **Question Generation**: Use an LLM to generate broad and detailed questions about the document.\n",
        "3. **Chunking Application**: Apply various chunking methods to the document. The next steps will be executed for each chunking strategy separately.\n",
        "4. **Chunk-Question Relevancy Scoring**: For each generated question, instruct an LLM to grade all chunks by relevancy to the question. These chunks become the ground truth and will represent our silver evaluation dataset.\n",
        "5. **Human Annotation**: Human annotators then review and modify the silver dataset to produce the gold dataset.\n",
        "6. **Chunk Retrieval**: Embed the chunks and retrieve the most similar chunks to each question.\n",
        "7. **Evaluate Retrieval**: Compare the retrieved chunks with the synthesized ground truth chunks and calculate the Precision, Recall and nDCG to analyse the retrieval performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python -1.-1.-1)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import os\n",
        "from typing import List, Dict, TypedDict\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import openai\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.schema import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded = load_dotenv(override=True)\n",
        "\n",
        "data_dir = \"data/\"\n",
        "os.environ['CHUNKING_BENCHMARK_DATADIR'] = data_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Load and Save Documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each document is loaded as one Langchain document possibly to small to fit into a LLM. Therefore, we need to split these documents into smaller pieces of text for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.data_loader import save_documents\n",
        "\n",
        "documents: List[Document] = []\n",
        "for file in os.listdir(data_dir+\"documents\"):\n",
        "    file_path = os.path.join(data_dir+\"documents\", file)\n",
        "    loader = TextLoader(file_path)\n",
        "    documents.extend(loader.load())\n",
        "\n",
        "save_documents(documents, data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.data_loader import load_documents\n",
        "documents = load_documents(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Question Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate synthetic Questions across Documents to challenge chunking strategies on multi-context queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question generation based on documents\n",
        "class Question(BaseModel):\n",
        "    question: str = Field(description=\"The question generated by the model\")\n",
        "    type: str = Field(description=\"The type of question generated\")\n",
        "\n",
        "class Questions(BaseModel):\n",
        "    questions: List[Question] = Field(description=\"The list of questions generated by the model\")\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=Questions)\n",
        "\n",
        "question_generation_prompt = PromptTemplate(\n",
        "    input_variables=[\"document\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "    template=\"\"\"\n",
        "You are a highly knowledgeable assistant tasked with generating challenging questions to evaluate different document chunking strategies in retrieval augmented generation (RAG) pipelines. \n",
        "\n",
        "<document>\n",
        "{document}\n",
        "</document>\n",
        "\n",
        "Based on the document provided, generate a set of questions that meet the following criteria:\n",
        "1. **Specific and Detailed Question (type: detailed):**  \n",
        "   - This question should be highly specific and precise, targeting one particular section, name, date, event, or factual detail within the document.\n",
        "   - The question should be designed to challenge chunking strategies that include too much context, potentially leading to noise or irrelevant information being retrieved. The answer should require precise retrieval of a specific detail without confusion from surrounding content.\n",
        "2. **Broad and Complex Question (type: broad):**  \n",
        "   - This question should be complex, requiring a comprehensive understanding of the document as a whole, involving multiple interlinked facts or concepts.\n",
        "   - The question should challenge chunking strategies that use chunks that are too small, making it difficult to retrieve and synthesize a well-rounded answer from fragmented pieces of information. The answer should require a broad perspective and the ability to connect multiple sections of the document.\n",
        "\n",
        "For each question type please generate 2 such questions about the provided document. Use the following format:\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "relevancy_chain = question_generation_prompt | generator_llm | parser\n",
        "questions = []\n",
        "for document in documents[:6]:\n",
        "    doc_questions = relevancy_chain.invoke({\"document\": document.page_content})[\"questions\"]\n",
        "    for question in doc_questions:\n",
        "        question[\"source\"] = document.metadata[\"source\"]\n",
        "\n",
        "    questions.extend(doc_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f\"{data_dir}synthetic_questions.json\", \"w\") as f:\n",
        "    json.dump(questions, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Apply chunking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute another notebook containing various chunking strategies which are applied on the documents. You can also run this notebook seperately to have manuel control over each method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run -i chunking_strategies.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load split chunks from disk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.data_loader import load_chunks\n",
        "split_chunks: Dict[str, Document] = load_chunks(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyze chunk count and average chunk size per strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_4e8b1\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_4e8b1_level0_col0\" class=\"col_heading level0 col0\" >Experiment</th>\n",
              "      <th id=\"T_4e8b1_level0_col1\" class=\"col_heading level0 col1\" >Chunk Count</th>\n",
              "      <th id=\"T_4e8b1_level0_col2\" class=\"col_heading level0 col2\" >Average Chunk Size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row0_col0\" class=\"data row0 col0\" >fixed_size-2048-0</td>\n",
              "      <td id=\"T_4e8b1_row0_col1\" class=\"data row0 col1\" >65</td>\n",
              "      <td id=\"T_4e8b1_row0_col2\" class=\"data row0 col2\" >1915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row1_col0\" class=\"data row1 col0\" >semantic_chunks_95</td>\n",
              "      <td id=\"T_4e8b1_row1_col1\" class=\"data row1 col1\" >67</td>\n",
              "      <td id=\"T_4e8b1_row1_col2\" class=\"data row1 col2\" >1848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row2_col0\" class=\"data row2 col0\" >fixed_size-2048-200</td>\n",
              "      <td id=\"T_4e8b1_row2_col1\" class=\"data row2 col1\" >72</td>\n",
              "      <td id=\"T_4e8b1_row2_col2\" class=\"data row2 col2\" >1902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row3_col0\" class=\"data row3 col0\" >recursive-2048-0</td>\n",
              "      <td id=\"T_4e8b1_row3_col1\" class=\"data row3 col1\" >72</td>\n",
              "      <td id=\"T_4e8b1_row3_col2\" class=\"data row3 col2\" >1728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row4_col0\" class=\"data row4 col0\" >recursive-2048-200</td>\n",
              "      <td id=\"T_4e8b1_row4_col1\" class=\"data row4 col1\" >73</td>\n",
              "      <td id=\"T_4e8b1_row4_col2\" class=\"data row4 col2\" >1736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row5_col0\" class=\"data row5 col0\" >semantic_chunks_90</td>\n",
              "      <td id=\"T_4e8b1_row5_col1\" class=\"data row5 col1\" >118</td>\n",
              "      <td id=\"T_4e8b1_row5_col2\" class=\"data row5 col2\" >1049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row6_col0\" class=\"data row6 col0\" >fixed_size-1024-0</td>\n",
              "      <td id=\"T_4e8b1_row6_col1\" class=\"data row6 col1\" >127</td>\n",
              "      <td id=\"T_4e8b1_row6_col2\" class=\"data row6 col2\" >980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row7_col0\" class=\"data row7 col0\" >markdown_header_parent</td>\n",
              "      <td id=\"T_4e8b1_row7_col1\" class=\"data row7 col1\" >146</td>\n",
              "      <td id=\"T_4e8b1_row7_col2\" class=\"data row7 col2\" >873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row8_col0\" class=\"data row8 col0\" >markdown_header</td>\n",
              "      <td id=\"T_4e8b1_row8_col1\" class=\"data row8 col1\" >146</td>\n",
              "      <td id=\"T_4e8b1_row8_col2\" class=\"data row8 col2\" >844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row9_col0\" class=\"data row9 col0\" >fixed_size-1024-200</td>\n",
              "      <td id=\"T_4e8b1_row9_col1\" class=\"data row9 col1\" >153</td>\n",
              "      <td id=\"T_4e8b1_row9_col2\" class=\"data row9 col2\" >1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row10_col0\" class=\"data row10 col0\" >recursive-1024-0</td>\n",
              "      <td id=\"T_4e8b1_row10_col1\" class=\"data row10 col1\" >155</td>\n",
              "      <td id=\"T_4e8b1_row10_col2\" class=\"data row10 col2\" >802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row11_col0\" class=\"data row11 col0\" >recursive-1024-200</td>\n",
              "      <td id=\"T_4e8b1_row11_col1\" class=\"data row11 col1\" >164</td>\n",
              "      <td id=\"T_4e8b1_row11_col2\" class=\"data row11 col2\" >810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row12_col0\" class=\"data row12 col0\" >fixed_size-512-0</td>\n",
              "      <td id=\"T_4e8b1_row12_col1\" class=\"data row12 col1\" >248</td>\n",
              "      <td id=\"T_4e8b1_row12_col2\" class=\"data row12 col2\" >502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row13_col0\" class=\"data row13 col0\" >recursive-512-0</td>\n",
              "      <td id=\"T_4e8b1_row13_col1\" class=\"data row13 col1\" >353</td>\n",
              "      <td id=\"T_4e8b1_row13_col2\" class=\"data row13 col2\" >351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row14_col0\" class=\"data row14 col0\" >recursive-512-200</td>\n",
              "      <td id=\"T_4e8b1_row14_col1\" class=\"data row14 col1\" >379</td>\n",
              "      <td id=\"T_4e8b1_row14_col2\" class=\"data row14 col2\" >378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4e8b1_row15_col0\" class=\"data row15 col0\" >fixed_size-512-200</td>\n",
              "      <td id=\"T_4e8b1_row15_col1\" class=\"data row15 col1\" >398</td>\n",
              "      <td id=\"T_4e8b1_row15_col2\" class=\"data row15 col2\" >507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x14941fef0>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(columns=[\"Experiment\", \"Chunk Count\", \"Average Chunk Size\"])\n",
        "for experiment_name, chunks in split_chunks.items():\n",
        "    df.loc[len(df)] = [experiment_name, len(chunks), round(sum([len(chunk.page_content) for chunk in chunks])/len(chunks))]\n",
        "\n",
        "df.sort_values(by=\"Chunk Count\", ascending=True).style.hide(axis=\"index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. (5.) Create Evaluation Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Relevancy Score for each chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f\"{data_dir}synthetic_questions.json\", \"r\") as f:\n",
        "    questions = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize datasets with questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.metrics import Testset\n",
        "\n",
        "datasets: Dict[str, List[Testset]]  = {}\n",
        "for experiment_name in split_chunks.keys():\n",
        "    datasets[experiment_name] = []\n",
        "    for question in questions:\n",
        "        datasets[experiment_name].append({\n",
        "            \"question\": question[\"question\"],\n",
        "            \"source\": question[\"source\"],\n",
        "            \"type\": question[\"type\"],\n",
        "            \"ground_truth_chunks\": {} \n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Relevancy Prompt is taken by Trulens. The difference is that I apply it to all chunks whereas Trulens only computed it on the retrieved chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gen_ai_hub.proxy.langchain import init_llm\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "class Grading(BaseModel):\n",
        "    score: float = Field(description=\"The score provided by the grader\")\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=Grading)\n",
        "\n",
        "judge_chunk_relevancy_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"context\", \"document\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "    template=\"\"\"\n",
        "You are a RELEVANCE grader. Given a CONTEXT from a DOCUMENT and a QUESTION, provide a score measuring the relevance between the CONTEXT and the QUESTION.\n",
        "The DOCUMENT acts as the ground truth and contains all necessary information to answer the QUESTION fully.\n",
        "The **RELEVANCE score** evaluates how well the CONTEXT helps answer the specific QUESTION in comparison to other parts of the DOCUMENT.\n",
        "\n",
        "Scoring Guidelines (0-4):\n",
        "- **0**: The CONTEXT does not answer the QUESTION at all.\n",
        "- **1**: The CONTEXT provides very minimal relevant information that doesn't contribute meaningfully to answering the QUESTION.\n",
        "- **2**: The CONTEXT answers part of the QUESTION but misses key details or is lacking in clarity.\n",
        "- **3**: The CONTEXT provides a mostly complete answer, but may lack a small detail or nuanced information that would make it fully comprehensive.\n",
        "- **4**: The CONTEXT contains the direct and complete information needed to fully and thoroughly answer the QUESTION.\n",
        "\n",
        "<DOCUMENT>\n",
        "{document}\n",
        "</DOCUMENT>\n",
        "\n",
        "<QUESTION>\n",
        "{question}\n",
        "</QUESTION>\n",
        "\n",
        "<CONTEXT>\n",
        "{context}\n",
        "</CONTEXT>\n",
        "\n",
        "Respond in the following format:\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "judge_chunk_noise_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "    template=\"\"\"\n",
        "You are a NOISE grader. Given a CONTEXT and a QUESTION, provide a noise score reflecting the amount of **non-contributing** or **excessive** information in the CONTEXT that distracts from answering the QUESTION.\n",
        "\n",
        "# Scoring Guidelines (0-4):\n",
        "The CONTEXT contains...\n",
        "- **0**: no irrelevant or excessive information.\n",
        "- **1**: some (1 - 4 sentences) irrelevant or excessive information, but most content is useful.\n",
        "- **2**: a moderate amount (5 - 10 sentences) of irrelevant or excessive content.\n",
        "- **3**: a significant amount (more than 10 sentences) of irrelevant or excessive content, though some useful parts remain..\n",
        "- **4**: mostly (multiple paragraphs) irrelevant or excessive content, making it difficult to find the relevant information.\n",
        "\n",
        "# CONTEXT and QUESTION to be graded:\n",
        "<CONTEXT>\n",
        "{context}\n",
        "</CONTEXT>\n",
        "<QUESTION>\n",
        "{question}\n",
        "</QUESTION>\n",
        "\n",
        "Output the noise score in the following format:\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "critic_llm = init_llm(model_name=\"gpt-4o\", temperature=0)\n",
        "# critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "relevancy_chain = judge_chunk_relevancy_prompt | critic_llm | parser\n",
        "noise_chain = judge_chunk_noise_prompt | critic_llm | parser\n",
        "\n",
        "def make_request_with_backoff(question, context, document, retries=8):\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            relevancy_score = relevancy_chain.invoke({\"question\": question, \"context\": context, \"document\": document})[\"score\"]\n",
        "            noise_score = 5\n",
        "            if relevancy_score != 0:\n",
        "                noise_score = noise_chain.invoke({\"question\": question, \"context\": context})[\"score\"]\n",
        "            return relevancy_score, noise_score\n",
        "        except openai.RateLimitError as e:\n",
        "            if i == retries - 1:\n",
        "                raise e\n",
        "            wait_time = 2**i\n",
        "            print(f\"Rate limited, waiting {wait_time} seconds\")\n",
        "            time.sleep(wait_time)\n",
        "        except openai.APIError as e:\n",
        "            print(e)\n",
        "\n",
        "\n",
        "def process_chunk(chunk, testset, document_content):\n",
        "    if chunk.metadata[\"source\"] not in testset[\"source\"]:\n",
        "        return None, None, None\n",
        "\n",
        "    chunk_relevancy, chunk_noise = make_request_with_backoff(testset[\"question\"], context=chunk.page_content, document=document_content)\n",
        "    if chunk_relevancy != 0.0:\n",
        "        return str(chunk.metadata[\"id\"]), chunk_relevancy, chunk_noise\n",
        "    return None, None, None\n",
        "\n",
        "for experiment_name, questions in datasets.items():\n",
        "    if os.path.exists(f\"{data_dir}/datasets/{experiment_name}.json\") or experiment_name != \"recursive-512-0\":\n",
        "        continue\n",
        "\n",
        "    print(\"Collecting ground truth for\", experiment_name)\n",
        "    for testset in tqdm(questions):\n",
        "        document_content = \"\"\n",
        "        for document in documents:\n",
        "            if document.metadata[\"source\"] == testset[\"source\"]:\n",
        "                document_content = document.page_content\n",
        "                break\n",
        "        ground_truth = {}\n",
        "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "            future_to_chunk = {\n",
        "                executor.submit(process_chunk, chunk, testset, document_content): chunk\n",
        "                for chunk in split_chunks[experiment_name]\n",
        "            }\n",
        "            for future in as_completed(future_to_chunk):\n",
        "                chunk_id, relevancy, noise = future.result()\n",
        "                if chunk_id and relevancy:\n",
        "                    ground_truth[chunk_id] = (relevancy, noise)\n",
        "        \n",
        "        if len(ground_truth):\n",
        "            testset[\"ground_truth_chunks\"] = ground_truth\n",
        "\n",
        "    with open(f\"{data_dir}/datasets/{experiment_name}.json\", \"w\") as f:\n",
        "        json.dump(questions, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyse dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_ceb0c\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_ceb0c_level0_col0\" class=\"col_heading level0 col0\" >Experiment</th>\n",
              "      <th id=\"T_ceb0c_level0_col1\" class=\"col_heading level0 col1\" >Chunk Count</th>\n",
              "      <th id=\"T_ceb0c_level0_col2\" class=\"col_heading level0 col2\" >Average Chunk Size</th>\n",
              "      <th id=\"T_ceb0c_level0_col3\" class=\"col_heading level0 col3\" >Average Ground Truth Count</th>\n",
              "      <th id=\"T_ceb0c_level0_col4\" class=\"col_heading level0 col4\" >Average Relevancy per Ground Truth 0-4</th>\n",
              "      <th id=\"T_ceb0c_level0_col5\" class=\"col_heading level0 col5\" >Average Noise per Ground Truth 0-5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_ceb0c_row0_col0\" class=\"data row0 col0\" >markdown_header_parent</td>\n",
              "      <td id=\"T_ceb0c_row0_col1\" class=\"data row0 col1\" >146</td>\n",
              "      <td id=\"T_ceb0c_row0_col2\" class=\"data row0 col2\" >873</td>\n",
              "      <td id=\"T_ceb0c_row0_col3\" class=\"data row0 col3\" >2.625000</td>\n",
              "      <td id=\"T_ceb0c_row0_col4\" class=\"data row0 col4\" >2.492063</td>\n",
              "      <td id=\"T_ceb0c_row0_col5\" class=\"data row0 col5\" >2.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_ceb0c_row1_col0\" class=\"data row1 col0\" >recursive-512-0</td>\n",
              "      <td id=\"T_ceb0c_row1_col1\" class=\"data row1 col1\" >353</td>\n",
              "      <td id=\"T_ceb0c_row1_col2\" class=\"data row1 col2\" >351</td>\n",
              "      <td id=\"T_ceb0c_row1_col3\" class=\"data row1 col3\" >5.916667</td>\n",
              "      <td id=\"T_ceb0c_row1_col4\" class=\"data row1 col4\" >1.718310</td>\n",
              "      <td id=\"T_ceb0c_row1_col5\" class=\"data row1 col5\" >3.140845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x17a0e5a90>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from utils.data_loader import load_datasets\n",
        "datasets = load_datasets(data_dir)\n",
        "\n",
        "df = pd.DataFrame(columns=[ \"Experiment\", \"Chunk Count\", \"Average Chunk Size\", \"Average Ground Truth Count\",\"Average Relevancy per Ground Truth 0-4\", \"Average Noise per Ground Truth 0-5\"])\n",
        "for experiment_name, questions in datasets.items():\n",
        "    total_relevancy = 0\n",
        "    total_noise = 0\n",
        "    total_ground_truth_count = 0\n",
        "    question_count = 0\n",
        "    for question in questions:\n",
        "        # if question[\"type\"] != \"detailed\":\n",
        "        #     continue\n",
        "        for chunk_relevancy, chunk_noise in question[\"ground_truth_chunks\"].values():\n",
        "            total_relevancy += chunk_relevancy\n",
        "            total_noise += chunk_noise\n",
        "        total_ground_truth_count += len(question[\"ground_truth_chunks\"])\n",
        "        question_count += 1\n",
        "    \n",
        "    average_ground_truth_count = total_ground_truth_count / question_count\n",
        "    average_relevancy_per_ground_truth = total_relevancy / total_ground_truth_count\n",
        "    average_noise_per_ground_truth = total_noise / total_ground_truth_count\n",
        "    average_chunk_size = round(sum([len(chunk.page_content) for chunk in split_chunks[experiment_name]]) / len(split_chunks[experiment_name]))\n",
        "    df.loc[len(df)] = [experiment_name, len(split_chunks[experiment_name]), average_chunk_size, average_ground_truth_count, average_relevancy_per_ground_truth, average_noise_per_ground_truth ]\n",
        "\n",
        "df.sort_values(by=\"Average Chunk Size\", ascending=False).style.hide(axis=\"index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Average Total Ground Truth Token Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3343"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_ground_truth_token_count = 0\n",
        "for experiment_name, questions in datasets.items():\n",
        "    ground_character_token_count = 0\n",
        "    for question in questions:\n",
        "        for split_chunk in split_chunks[experiment_name]:\n",
        "            if str(split_chunk.metadata[\"id\"]) in question[\"ground_truth_chunks\"]:\n",
        "                ground_character_token_count += len(split_chunk.page_content)\n",
        "    mean_ground_truth_token_count += ground_character_token_count / (len(questions)*4) # *4 because on token is approximately 4 characters\n",
        "\n",
        "round(mean_ground_truth_token_count / len(datasets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. - 7. Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingest Chunks into Vector Store\n",
        "\n",
        "Using FAISS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading recursive-512-0\n",
            "Loading markdown_header_parent\n",
            "Loading fixed_size-512-200\n",
            "Loading recursive-512-200\n",
            "Loading markdown_header\n",
            "Loading recursive-1024-200\n",
            "Loading recursive-1024-0\n",
            "Loading fixed_size-2048-200\n",
            "Loading fixed_size-2048-0\n",
            "Loading semantic_chunks_90\n",
            "Loading fixed_size-512-0\n",
            "Loading recursive-2048-200\n",
            "Loading fixed_size-1024-0\n",
            "Loading fixed_size-1024-200\n",
            "Loading semantic_chunks_95\n",
            "Loading recursive-2048-0\n"
          ]
        }
      ],
      "source": [
        "# from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "vector_stores: Dict[str, VectorStore] = {}\n",
        "\n",
        "# embeddings = HuggingFaceEmbeddings(\n",
        "#     model_name=\"Snowflake/snowflake-arctic-embed-l\",\n",
        "    # model_name=\"Alibaba-NLP/gte-large-en-v1.5\",\n",
        "#     model_kwargs={\"device\": 0, 'trust_remote_code': True},  # Comment out to use CPU\n",
        "# )\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "model_name = (embeddings.model_name if hasattr(embeddings, 'model_name') else embeddings.model).replace(\"/\", \"_\")\n",
        "vector_store_dir = f\"{data_dir}vector_stores/{model_name}\"\n",
        "Path(vector_store_dir).mkdir(parents=True, exist_ok=True)\n",
        "for experiment_name, chunks in split_chunks.items():\n",
        "    if os.path.exists(f\"{vector_store_dir}/{experiment_name}\"):\n",
        "        print(\"Loading\", experiment_name)\n",
        "        vector_stores[experiment_name] = FAISS.load_local(f\"{vector_store_dir}/{experiment_name}\", embeddings, allow_dangerous_deserialization=True)\n",
        "    else:\n",
        "        print(\"Indexing\", experiment_name)\n",
        "        vector_stores[experiment_name] = FAISS.from_documents(chunks, embeddings)\n",
        "        vector_stores[experiment_name].save_local(f\"{vector_store_dir}/{experiment_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select Evaluation Approach and load results if already exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EvalApproach:\n",
        "    FIXED_K = \"Fixed-K\"\n",
        "    GROUND_TRUTH_K = \"Ground-Truth-K\"\n",
        "    TOKEN_LIMIT = \"Token-Limit\"\n",
        "    RATIO_K = \"Ratio-K\"\n",
        "\n",
        "SEL_APPROACH: EvalApproach = EvalApproach.RATIO_K\n",
        "FIXED_K = 20\n",
        "TOKEN_LIMIT = 3340\n",
        "RATIO_K = 0.1\n",
        "\n",
        "model_name = \"text-embedding-3-small\"\n",
        "eval_name = f\"{SEL_APPROACH}-{FIXED_K}-{model_name}\" if SEL_APPROACH == EvalApproach.FIXED_K else \"\"\n",
        "eval_name = f\"{SEL_APPROACH}-{model_name}\" if SEL_APPROACH == EvalApproach.GROUND_TRUTH_K else eval_name\n",
        "eval_name = f\"{SEL_APPROACH}-{TOKEN_LIMIT}-{model_name}\" if SEL_APPROACH == EvalApproach.TOKEN_LIMIT else eval_name\n",
        "eval_name = f\"{SEL_APPROACH}-{RATIO_K}-{model_name}\" if SEL_APPROACH == EvalApproach.RATIO_K else eval_name\n",
        "if os.path.exists(f\"{data_dir}results/{eval_name}.csv\"):\n",
        "    results = pd.read_csv(f\"{data_dir}results/{eval_name}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate new results run the following. It calculates the retrieval metrics for the loaded dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.data_loader import load_datasets\n",
        "datasets = load_datasets(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating semantic_chunks_95 with K = 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:08<00:00,  2.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating recursive-2048-0 with K = 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:07<00:00,  3.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating markdown_header_parent with K = 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:06<00:00,  3.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating fixed_size-2048-0 with K = 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:07<00:00,  3.08it/s]\n"
          ]
        }
      ],
      "source": [
        "from utils.metrics import calculate_metrics, calculate_mean_metrics\n",
        "\n",
        "results_list = []\n",
        "for experiment_name, questions in datasets.items():\n",
        "    if experiment_name not in vector_stores:\n",
        "        continue\n",
        "\n",
        "    K = FIXED_K if SEL_APPROACH == EvalApproach.FIXED_K else 0\n",
        "    K = (\n",
        "        round(len(split_chunks[experiment_name]) * RATIO_K)\n",
        "        if SEL_APPROACH == EvalApproach.RATIO_K\n",
        "        else K\n",
        "    )\n",
        "    K = (\n",
        "        200 if SEL_APPROACH == EvalApproach.TOKEN_LIMIT else K\n",
        "    )  # large number to ensure TOKEN_LIMIT is always reached\n",
        "    print(\"Evaluating\", experiment_name, \"with K =\", K if K else \"Ground Truth based\")\n",
        "    metrics = []\n",
        "    for testset in tqdm(questions):\n",
        "        if testset[\"ground_truth_chunks\"] == {}:\n",
        "            continue\n",
        "        question = testset[\"question\"]\n",
        "        ground_truth = testset[\"ground_truth_chunks\"]\n",
        "        K = len(ground_truth) if SEL_APPROACH == EvalApproach.GROUND_TRUTH_K else K\n",
        "\n",
        "        retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": K})\n",
        "        retrieved_chunks = retriever.invoke(question)\n",
        "\n",
        "        if SEL_APPROACH == EvalApproach.TOKEN_LIMIT:\n",
        "            # cap the number of retrieved chunks where sum of page_contents are below a fixed context window\n",
        "            retrieved_chunks_capped = []\n",
        "            total_context_length = 0\n",
        "            for chunk in retrieved_chunks:\n",
        "                total_context_length += len(chunk.page_content)\n",
        "                if (\n",
        "                    total_context_length > TOKEN_LIMIT * 4\n",
        "                ):  # as one token on average is approximately 4 characters\n",
        "                    break\n",
        "                retrieved_chunks_capped.append(chunk)\n",
        "\n",
        "            retrieved_chunks = retrieved_chunks_capped\n",
        "\n",
        "        retrieved_chunk_ids = [str(doc.metadata[\"id\"]) for doc in retrieved_chunks]\n",
        "        metrics.append(\n",
        "            calculate_metrics(\n",
        "                retrieved_chunk_ids,\n",
        "                ground_truth_chunks=list(ground_truth.keys()),\n",
        "                ground_truth_relevancies=[relevancy for relevancy, _ in ground_truth.values()],\n",
        "                ground_truth_noises=[noise for _, noise in ground_truth.values()],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    mean_metrics = (\n",
        "        calculate_mean_metrics(metrics)\n",
        "        if len(metrics)\n",
        "        else {\n",
        "            \"precision\": 0.0,\n",
        "            \"noise\": 0.0,\n",
        "            \"recall\": 0.0,\n",
        "            \"map\": 0.0,\n",
        "            \"ndcg\": 0.0,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        experiment_chunk_size = int(experiment_name.split(\"-\")[-2])\n",
        "        experiment_chunk_overlap = int(experiment_name.split(\"-\")[-1])\n",
        "    except:\n",
        "        experiment_chunk_size = None\n",
        "        experiment_chunk_overlap = None\n",
        "\n",
        "    results_list.append(\n",
        "        [\n",
        "            experiment_name.split(\"-\")[0],\n",
        "            experiment_chunk_size,\n",
        "            experiment_chunk_overlap,\n",
        "            mean_metrics[\"precision\"],\n",
        "            mean_metrics[\"noise\"] / 5,\n",
        "            mean_metrics[\"recall\"],\n",
        "            mean_metrics[\"map\"],\n",
        "            mean_metrics[\"ndcg\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "results = pd.DataFrame(\n",
        "    results_list,\n",
        "    columns=[\n",
        "        eval_name,\n",
        "        \"Chunk Size\",\n",
        "        \"Chunk Overlap\",\n",
        "        \"Precision\",\n",
        "        \"Noise\",\n",
        "        \"Recall\",\n",
        "        \"MAP\",\n",
        "        \"NDCG\",\n",
        "    ],\n",
        ")\n",
        "results.to_csv(f\"{data_dir}results/{eval_name}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Chunking Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Noise</th>\n",
              "      <th>Recall</th>\n",
              "      <th>NDCG</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ratio-K-0.1-text-embedding-3-small</th>\n",
              "      <th>Chunk Size</th>\n",
              "      <th>Chunk Overlap</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>markdown_header_parent</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.928226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>semantic_chunks_95</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>0.309524</td>\n",
              "      <td>0.863095</td>\n",
              "      <td>0.994048</td>\n",
              "      <td>0.944802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fixed_size</th>\n",
              "      <th>2048.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.381944</td>\n",
              "      <td>0.829167</td>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.913412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recursive</th>\n",
              "      <th>2048.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.339286</td>\n",
              "      <td>0.840476</td>\n",
              "      <td>0.952083</td>\n",
              "      <td>0.874066</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             Precision  \\\n",
              "Ratio-K-0.1-text-embedding-3-small Chunk Size Chunk Overlap              \n",
              "markdown_header_parent             NaN        NaN             0.175000   \n",
              "semantic_chunks_95                 NaN        NaN             0.309524   \n",
              "fixed_size                         2048.0     0.0             0.381944   \n",
              "recursive                          2048.0     0.0             0.339286   \n",
              "\n",
              "                                                                Noise  \\\n",
              "Ratio-K-0.1-text-embedding-3-small Chunk Size Chunk Overlap             \n",
              "markdown_header_parent             NaN        NaN            0.911111   \n",
              "semantic_chunks_95                 NaN        NaN            0.863095   \n",
              "fixed_size                         2048.0     0.0            0.829167   \n",
              "recursive                          2048.0     0.0            0.840476   \n",
              "\n",
              "                                                               Recall  \\\n",
              "Ratio-K-0.1-text-embedding-3-small Chunk Size Chunk Overlap             \n",
              "markdown_header_parent             NaN        NaN            1.000000   \n",
              "semantic_chunks_95                 NaN        NaN            0.994048   \n",
              "fixed_size                         2048.0     0.0            0.986111   \n",
              "recursive                          2048.0     0.0            0.952083   \n",
              "\n",
              "                                                                 NDCG  \n",
              "Ratio-K-0.1-text-embedding-3-small Chunk Size Chunk Overlap            \n",
              "markdown_header_parent             NaN        NaN            0.928226  \n",
              "semantic_chunks_95                 NaN        NaN            0.944802  \n",
              "fixed_size                         2048.0     0.0            0.913412  \n",
              "recursive                          2048.0     0.0            0.874066  "
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results.drop(columns=[\"MAP\"]).groupby([eval_name, \"Chunk Size\", \"Chunk Overlap\"], dropna=False).mean().sort_values(by=\"Recall\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.799677814146692"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate F1 score\n",
        "results[\"F1\"] = 2 * (results[\"Precision\"] * results[\"Recall\"]) / (results[\"Precision\"] + results[\"Recall\"])\n",
        "results[\"F1\"].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Chunk Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Noise</th>\n",
              "      <th>Recall</th>\n",
              "      <th>NDCG</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Chunk Size</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NaN</th>\n",
              "      <td>0.768188</td>\n",
              "      <td>5.803704</td>\n",
              "      <td>0.878910</td>\n",
              "      <td>0.937948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512.0</th>\n",
              "      <td>0.751190</td>\n",
              "      <td>5.959524</td>\n",
              "      <td>0.838851</td>\n",
              "      <td>0.888149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2048.0</th>\n",
              "      <td>0.813161</td>\n",
              "      <td>5.455026</td>\n",
              "      <td>0.827984</td>\n",
              "      <td>0.942866</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Precision     Noise    Recall      NDCG\n",
              "Chunk Size                                         \n",
              "NaN          0.768188  5.803704  0.878910  0.937948\n",
              "512.0        0.751190  5.959524  0.838851  0.888149\n",
              "2048.0       0.813161  5.455026  0.827984  0.942866"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results.drop(columns=[eval_name, \"Chunk Overlap\", \"MAP\"]).groupby(\"Chunk Size\", dropna=False).mean().sort_values(by=\"Recall\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>NDCG</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Chunk Overlap</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>0.763958</td>\n",
              "      <td>0.844961</td>\n",
              "      <td>0.922739</td>\n",
              "      <td>0.802406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200.0</th>\n",
              "      <td>0.773269</td>\n",
              "      <td>0.838680</td>\n",
              "      <td>0.918079</td>\n",
              "      <td>0.804549</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Precision    Recall      NDCG        F1\n",
              "Chunk Overlap                                         \n",
              "0.0             0.763958  0.844961  0.922739  0.802406\n",
              "200.0           0.773269  0.838680  0.918079  0.804549"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results.where(results[eval_name] == \"recursive\").drop(columns=[eval_name, \"Chunk Size\", \"MAP\"]).groupby(\"Chunk Overlap\").mean().sort_values(by=\"NDCG\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Generation (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "answer_correctness_system_prompt = \"\"\"You are a CORRECTNESS grader; providing the correctness of the given GENERATED ANSWER compared to the given GROUND TRUTH ANSWER.\n",
        "Respond only as a number from 0 to 10 where 0 is the least correct and 10 is the most correct.\n",
        "\n",
        "A few additional scoring guidelines:\n",
        "\n",
        "- Long GENERATED ANSWERS should score equally well as short GENERATED ANSWERS.\n",
        "\n",
        "- CORRECTNESS score should increase as the GENERATED ANSWER matches more accurately with the GROUND TRUTH ANSWER.\n",
        "\n",
        "- CORRECTNESS score should increase as the GENERATED ANSWER covers more parts of the GROUND TRUTH ANSWER accurately.\n",
        "\n",
        "- GENERATED ANSWERS that partially match the GROUND TRUTH ANSWER should score 2, 3, or 4. Higher scores indicate more correctness.\n",
        "\n",
        "- GENERATED ANSWERS that mostly match the GROUND TRUTH ANSWER should get a score of 5, 6, 7, or 8. Higher scores indicate more correctness.\n",
        "\n",
        "- GENERATED ANSWERS that fully match the GROUND TRUTH ANSWER should get a score of 9 or 10. Higher scores indicate more correctness.\n",
        "\n",
        "- GENERATED ANSWERS must be fully accurate and comprehensive to the GROUND TRUTH ANSWER to get a score of 10.\n",
        "\n",
        "- Never elaborate.\"\"\"\n",
        "\n",
        "answer_correctness_user_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"GROUND TRUTH ANSWER: {ground_truth_answer}\n",
        "\n",
        "GENERATED ANSWER: {generated_answer}\n",
        "\n",
        "CORRECTNESS: \"\"\"\n",
        ")\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "        \n",
        "for experiment_name, questions in datasets.items():\n",
        "    print(\"Evaluating\", experiment_name)\n",
        "    vector_stores[experiment_name].embeddings.show_progress_bar = False\n",
        "    retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": 10})\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | generator_llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    for question_type, datasets in questions.items():\n",
        "        mean_answer_correctness = 0\n",
        "        for testset in datasets:\n",
        "            response = rag_chain.invoke(testset[\"question\"])\n",
        "            answer_correctness_prompt = answer_correctness_user_prompt.format(\n",
        "                ground_truth_answer=testset[\"ground_truth_answer\"], generated_answer=response\n",
        "            )\n",
        "\n",
        "            llm_messages = [\n",
        "                SystemMessage(content=answer_correctness_system_prompt),\n",
        "                HumanMessage(content=answer_correctness_prompt),\n",
        "            ]\n",
        "            response = make_request_with_backoff(llm_messages)\n",
        "\n",
        "            answer_correctness = re_0_10_rating(response.content)\n",
        "            mean_answer_correctness += answer_correctness\n",
        "        mean_answer_correctness /= len(datasets)\n",
        "        print(f\"Experiment: {experiment_name} Question Type: {question_type} Mean Answer Correctness: {mean_answer_correctness}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "-1.-1.-1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
