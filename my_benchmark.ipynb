{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Evaluation Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/my_approach.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, TypedDict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_dotenv()\n",
    "\n",
    "data_dir = \"my_benchmark/\"\n",
    "os.environ['CHUNKING_BENCHMARK'] = data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and Save Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document is loaded as one Langchain document possibly to small to fit into a LLM. Therefore, we need to split these documents into smaller pieces of text for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import save_documents\n",
    "\n",
    "documents: List[Document] = []\n",
    "for file in os.listdir(data_dir+\"documents\"):\n",
    "    file_path = os.path.join(data_dir+\"documents\", file)\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "save_documents(documents, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_documents\n",
    "documents = load_documents(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i chunking_strategies.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_chunks\n",
    "split_chunks: Dict[str, Document] = load_chunks(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ingest into vector store\n",
    "\n",
    "Using FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_stores: Dict[str, VectorStore] = {}\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", show_progress_bar=True)\n",
    "for experiment_name, chunks in split_chunks.items():\n",
    "    if os.path.exists(f\"{data_dir}vector_stores/{experiment_name}\"):\n",
    "        print(\"Loading\", experiment_name)\n",
    "        vector_stores[experiment_name] = FAISS.load_local(f\"{data_dir}vector_stores/{experiment_name}\", embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        print(\"Indexing\", experiment_name)\n",
    "        vector_stores[experiment_name] = FAISS.from_documents(chunks, embeddings)\n",
    "        vector_stores[experiment_name].save_local(f\"{data_dir}vector_stores/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Evaluation Datasets for each Chunking Strategy should include the following:\n",
    "\n",
    "- Questions across Documents\n",
    "- Ground Truth Chunks (with graded Relevance)\n",
    "- Ground Truth Answers\n",
    "\n",
    "For Simple, Reasoning and Multi-Context Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import GoldenTestset\n",
    "\n",
    "class Questions(TypedDict):\n",
    "    simple: List[GoldenTestset]\n",
    "    reasoning: List[GoldenTestset]\n",
    "    multi_context: List[GoldenTestset]\n",
    "\n",
    "gold_dataset: Dict[str, Questions]  = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation with RAGAS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate synthetic Questions across Documents to challenge chunking strategies on multi-context queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "environ[\"RAGAS_DO_NOT_TRACK\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)\n",
    "\n",
    "ragas_testset = generator.generate_with_langchain_docs(\n",
    "    documents,\n",
    "    test_size=10,\n",
    "    distributions={simple: 0.3, reasoning: 0.3, multi_context: 0.4},\n",
    ")\n",
    "df = ragas_testset.to_pandas()\n",
    "df.to_csv(data_dir+\"ragas_testset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_testset = pd.read_csv(data_dir+\"ragas_testset.csv\")\n",
    "for experiment_name in split_chunks.keys():\n",
    "    gold_dataset[experiment_name] = {\n",
    "        \"simple\": [],\n",
    "        \"reasoning\": [],\n",
    "        \"multi_context\": []\n",
    "    }\n",
    "    for _, row in ragas_testset.iterrows():\n",
    "        testset = {\n",
    "            \"question\": row['question'],\n",
    "            \"ground_truth_chunks\": [],\n",
    "            \"ground_truth_answer\": row['ground_truth']\n",
    "        }\n",
    "        gold_dataset[experiment_name][row[\"evolution_type\"]].append(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Relevancy Score for each chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevancy Prompt is taken by Trulens. The difference is that I apply it to all chunks whereas Trulens only computed it on the retrieved chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.llm_output_parser import re_0_10_rating\n",
    "\n",
    "system_prompt = \"\"\"You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\n",
    "    Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
    "\n",
    "    A few additional scoring guidelines:\n",
    "\n",
    "    - Long CONTEXTS should score equally well as short CONTEXTS.\n",
    "\n",
    "    - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\n",
    "\n",
    "    - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
    "\n",
    "    - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
    "\n",
    "    - Never elaborate.\"\"\"\n",
    "\n",
    "user_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"QUESTION: {question}\n",
    "\n",
    "    CONTEXT: {context}\n",
    "    \n",
    "    RELEVANCE: \"\"\"\n",
    ")\n",
    "\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def make_request_with_backoff(messages, retries=8):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = critic_llm.invoke(messages)\n",
    "            return response\n",
    "        except openai.RateLimitError as e:\n",
    "            if i == retries - 1:\n",
    "                raise e\n",
    "            wait_time = 2**i\n",
    "            print(f\"Rate limit error, retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "\n",
    "for experiment_name, questions in gold_dataset.items():\n",
    "    print(\"Collecting ground truth for\", experiment_name)\n",
    "    for question_type, testsets in questions.items():\n",
    "        for testset in testsets:\n",
    "            print(\"Collecting ground truth for\", testset[\"question\"])\n",
    "            ground_truth = {}\n",
    "            for chunk in tqdm(split_chunks[experiment_name]):\n",
    "                judge_chunk_relevancy_prompt = user_prompt.format(\n",
    "                    question=testset[\"question\"], context=chunk.page_content\n",
    "                )\n",
    "\n",
    "                llm_messages = [\n",
    "                    SystemMessage(content=system_prompt),\n",
    "                    HumanMessage(content=judge_chunk_relevancy_prompt),\n",
    "                ]\n",
    "                response = make_request_with_backoff(llm_messages)\n",
    "\n",
    "                chunk_relevancy = re_0_10_rating(response.content)\n",
    "                if chunk_relevancy != 0.0:\n",
    "                    ground_truth[str(chunk.metadata[\"id\"])] = chunk_relevancy\n",
    "            testset[\"ground_truth_chunks\"] = ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Evaluation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir+'gold_dataset.json', 'w') as jsonl_file:\n",
    "    json.dump(gold_dataset, jsonl_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Evaluation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_dataset = {}\n",
    "with open(data_dir+'gold_dataset.json', 'r') as jsonl_file:\n",
    "    gold_dataset = json.load(jsonl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import calculate_metrics, calculate_mean_metrics\n",
    "\n",
    "for experiment_name, questions in gold_dataset.items():\n",
    "    if experiment_name not in vector_stores:\n",
    "        continue\n",
    "    \n",
    "    # K = 30\n",
    "    # vector_stores[experiment_name].embeddings.show_progress_bar = False\n",
    "    # retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": K})\n",
    "    for question_type, testsets in questions.items():\n",
    "        metrics = []\n",
    "        for testset in testsets:\n",
    "            question = testset[\"question\"]\n",
    "            # filter out ground truth chunks that have a low relevance score\n",
    "            ground_truth = {k: relevance for k, relevance in testset[\"ground_truth_chunks\"].items() if relevance > 2}\n",
    "            K = len(ground_truth)\n",
    "            vector_stores[experiment_name].embeddings.show_progress_bar = False\n",
    "            retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": K})\n",
    "            retrieved_chunks = retriever.invoke(question)\n",
    "            retrieved_chunk_ids = [str(doc.metadata[\"id\"]) for doc in retrieved_chunks]\n",
    "            metrics.append(calculate_metrics(retrieved_chunk_ids, ground_truth_chunks=list(ground_truth.keys()), ground_truth_relevancies=list(ground_truth.values()), K=10))\n",
    "        \n",
    "        mean_metrics = calculate_mean_metrics(metrics)\n",
    "        print(f\"Mean metrics for {experiment_name} {question_type}: {mean_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "answer_correctness_system_prompt = \"\"\"You are a CORRECTNESS grader; providing the correctness of the given GENERATED ANSWER compared to the given GROUND TRUTH ANSWER.\n",
    "Respond only as a number from 0 to 10 where 0 is the least correct and 10 is the most correct.\n",
    "\n",
    "A few additional scoring guidelines:\n",
    "\n",
    "- Long GENERATED ANSWERS should score equally well as short GENERATED ANSWERS.\n",
    "\n",
    "- CORRECTNESS score should increase as the GENERATED ANSWER matches more accurately with the GROUND TRUTH ANSWER.\n",
    "\n",
    "- CORRECTNESS score should increase as the GENERATED ANSWER covers more parts of the GROUND TRUTH ANSWER accurately.\n",
    "\n",
    "- GENERATED ANSWERS that partially match the GROUND TRUTH ANSWER should score 2, 3, or 4. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS that mostly match the GROUND TRUTH ANSWER should get a score of 5, 6, 7, or 8. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS that fully match the GROUND TRUTH ANSWER should get a score of 9 or 10. Higher scores indicate more correctness.\n",
    "\n",
    "- GENERATED ANSWERS must be fully accurate and comprehensive to the GROUND TRUTH ANSWER to get a score of 10.\n",
    "\n",
    "- Never elaborate.\"\"\"\n",
    "\n",
    "answer_correctness_user_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"GROUND TRUTH ANSWER: {ground_truth_answer}\n",
    "\n",
    "GENERATED ANSWER: {generated_answer}\n",
    "\n",
    "CORRECTNESS: \"\"\"\n",
    ")\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        \n",
    "for experiment_name, questions in gold_dataset.items():\n",
    "    print(\"Evaluating\", experiment_name)\n",
    "    vector_stores[experiment_name].embeddings.show_progress_bar = False\n",
    "    retriever = vector_stores[experiment_name].as_retriever(search_kwargs={\"k\": 10})\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | generator_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    for question_type, testsets in questions.items():\n",
    "        mean_answer_correctness = 0\n",
    "        for testset in testsets:\n",
    "            response = rag_chain.invoke(testset[\"question\"])\n",
    "            answer_correctness_prompt = answer_correctness_user_prompt.format(\n",
    "                ground_truth_answer=testset[\"ground_truth_answer\"], generated_answer=response\n",
    "            )\n",
    "\n",
    "            llm_messages = [\n",
    "                SystemMessage(content=answer_correctness_system_prompt),\n",
    "                HumanMessage(content=answer_correctness_prompt),\n",
    "            ]\n",
    "            response = make_request_with_backoff(llm_messages)\n",
    "\n",
    "            answer_correctness = re_0_10_rating(response.content)\n",
    "            mean_answer_correctness += answer_correctness\n",
    "        mean_answer_correctness /= len(testsets)\n",
    "        print(f\"Experiment: {experiment_name} Question Type: {question_type} Mean Answer Correctness: {mean_answer_correctness}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
