{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Hop RAG Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/multi_hop_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question and Ground Truths are provided by the Multi Hop RAG Benchmark\n",
    "Hard to use as ground truths consist of facts (sentences) that could have been split up by chunking strategies. But maybe not so bad as sentences shouldnt be split up anyways. LOl\n",
    "\n",
    "**CANT use gpt-4o-mini because knowledge cutoff is in 2023 so it may be able to answer qeury without needing RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, TypedDict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import tiktoken\n",
    "import openai\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_dotenv()\n",
    "\n",
    "data_dir = \"data/multi_hop_rag/\"\n",
    "os.environ['CHUNKING_BENCHMARK_DATADIR'] = data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and Save Multi Hop RAG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import save_documents\n",
    "\n",
    "documents: List[Document] = []\n",
    "with open(data_dir+\"dataset/corpus.json\", 'r') as file_name:\n",
    "    load_data = json.load(file_name)\n",
    "\n",
    "for data in load_data:\n",
    "    metadata = {\"title\": data['title'], \"published_at\": data['published_at'],\"source\":data['source']}\n",
    "    documents.append(Document(page_content=data['body'], metadata=metadata))\n",
    "\n",
    "save_documents(documents, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "num_tokens = 0\n",
    "for doc in documents:\n",
    "    num_tokens += len(encoding.encode(doc.page_content))\n",
    "\n",
    "cost = (num_tokens/1000000) * 0.01\n",
    "print(f\"Cost of embedding chunks: {cost} with {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_documents\n",
    "documents = load_documents(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chunking_strategies.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import load_chunks\n",
    "split_chunks = load_chunks(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Indexing/Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fixed-size-1000-0\n",
      "Loading recursive-1000-0\n",
      "Loading semantic-chunks\n"
     ]
    }
   ],
   "source": [
    "vector_stores: Dict[str, VectorStore] = {}\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", show_progress_bar=True)\n",
    "for experiment_name, chunks in split_chunks.items():\n",
    "    if os.path.exists(f\"{data_dir}vector_stores/{experiment_name}\"):\n",
    "        print(\"Loading\", experiment_name)\n",
    "        vector_stores[experiment_name] = FAISS.load_local(f\"{data_dir}vector_stores/{experiment_name}\", embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        print(\"Indexing\", experiment_name)\n",
    "        vector_stores[experiment_name] = FAISS.from_documents(chunks, embeddings)\n",
    "        vector_stores[experiment_name].save_local(f\"{data_dir}vector_stores/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"multi_hop_rag/dataset/MultiHopRAG.json\", \"r\") as file_name:\n",
    "    query_data = json.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "num_tokens = 0\n",
    "for data in query_data:\n",
    "    num_tokens += len(encoding.encode(data['query']))\n",
    "\n",
    "cost = (num_tokens/1000000) * 0.01\n",
    "print(f\"Cost of embedding chunks: {cost} with {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating evaluation dataset for fixed-size-1000-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:30<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean metrics for fixed-size-1000-0 {'precision': 0.19230769230769232, 'recall': 0.19230769230769232, 'map': 0.15018315018315018, 'ndcg': 0.0}\n",
      "Generating evaluation dataset for recursive-1000-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean metrics for recursive-1000-0 {'precision': 0.27289377289377287, 'recall': 0.27289377289377287, 'map': 0.22115384615384612, 'ndcg': 0.0}\n",
      "Generating evaluation dataset for semantic-chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:29<00:00,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean metrics for semantic-chunks {'precision': 0.28388278388278393, 'recall': 0.28388278388278393, 'map': 0.22779304029304026, 'ndcg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.evaluation import calculate_metrics, calculate_mean_metrics\n",
    "\n",
    "for experiment_name, vector_store in vector_stores.items():\n",
    "    print(\"Generating evaluation dataset for\", experiment_name)\n",
    "    # vector_store.embeddings.show_progress_bar = False\n",
    "    # retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "    metrics = []\n",
    "    for data in tqdm(query_data[:100]):\n",
    "        if data['question_type'] == 'null_query':\n",
    "            continue\n",
    "        query = data[\"query\"]\n",
    "        vector_store.embeddings.show_progress_bar = False\n",
    "        retriever = vector_store.as_retriever(search_kwargs={\"k\": len(data[\"evidence_list\"])})\n",
    "        retrieved_chunks = retriever.invoke(query)\n",
    "        retrieved_chunks_content = ([doc.page_content for doc in retrieved_chunks])\n",
    "        ground_truths = {gold[\"fact\"]: 1.0 for gold in data[\"evidence_list\"]}\n",
    "        metrics.append(calculate_metrics(retrieved_chunks_content, ground_truths))\n",
    "    mean_metrics = calculate_mean_metrics(metrics)\n",
    "    print(\"Mean metrics for\", experiment_name, mean_metrics)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "evaluation_datasets = {}\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        \n",
    "for experiment_name, vector_store in vector_stores.items():\n",
    "    print(\"Evaluating\", experiment_name)\n",
    "    evaluation_datasets[experiment_name] = { \"question\": [], \"answer\": [], \"ground_truth\": [] }\n",
    "    vector_store.embeddings.show_progress_bar = False\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | generator_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    for data in tqdm(query_data[:10]):\n",
    "        if data['question_type'] == 'null_query':\n",
    "            continue\n",
    "        query = data[\"query\"]\n",
    "        response = rag_chain.invoke(query)\n",
    "        evaluation_datasets[experiment_name][\"question\"].append(query)\n",
    "        evaluation_datasets[experiment_name][\"answer\"].append(response)\n",
    "        evaluation_datasets[experiment_name][\"ground_truth\"].append(data[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "from ragas.metrics import answer_correctness\n",
    "from ragas import evaluate\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "for experiment_name, data in evaluation_datasets.items():\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    score = evaluate(dataset,metrics=[answer_correctness], llm=critic_llm)\n",
    "    answer_correctnesses = score.to_pandas()[\"answer_correctness\"].tolist()\n",
    "    data[\"answer_correctnesses\"] = answer_correctnesses\n",
    "    print(f\"Answer correctness for {experiment_name}: {score.values}\")\n",
    "\n",
    "    # data[\"answer_correctness\"] = score\n",
    "    # print(score.to_pandas())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
